{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "def get_image_and_labels(path1,path2,origin):\n",
    "    train_image_list = []\n",
    "    train_label_list = []\n",
    "    val_image_list = []\n",
    "    val_label_list = []\n",
    "    \n",
    "    if origin:\n",
    "        raw_train_labels = pd.read_csv('trainOrigin.csv')\n",
    "        raw_val_labels = pd.read_csv('testOrigin.csv')\n",
    "    else:\n",
    "        raw_train_labels = pd.read_csv('train.csv')\n",
    "        raw_val_labels = pd.read_csv('test.csv')\n",
    "    \n",
    "    for i in os.listdir(path1):\n",
    "        train_image_list.append(path1+'/'+i)\n",
    "        train_label_list.append(int(raw_train_labels[raw_train_labels['ImageId']==i]['ship_or_not'].values[0]))\n",
    "    \n",
    "    for i in os.listdir(path2):\n",
    "        val_image_list.append(path2+'/'+i)\n",
    "        val_label_list.append(int(raw_val_labels[raw_val_labels['ImageId']==i]['ship_or_not'].values[0]))\n",
    "    \n",
    "    temp = np.array([train_image_list,train_label_list])\n",
    "    temp = np.transpose(temp)\n",
    "    np.random.shuffle(temp)\n",
    "    \n",
    "    train_image_list = temp[:,0]\n",
    "    train_label_list = temp[:,1]\n",
    "    \n",
    "    train_label_list = [int(i) for i in train_label_list]    \n",
    "    train_label_list = tf.cast(train_label_list, tf.int32)\n",
    "    \n",
    "    temp = np.array([val_image_list,val_label_list])\n",
    "    temp = np.transpose(temp)\n",
    "    np.random.shuffle(temp)\n",
    "    \n",
    "    val_image_list = temp[:,0]\n",
    "    val_label_list = temp[:,1]\n",
    "    \n",
    "    val_label_list = [int(i) for i in val_label_list]\n",
    "    val_label_list = tf.cast(val_label_list, tf.int32)\n",
    "\n",
    "    return train_image_list,train_label_list,val_image_list,val_label_list\n",
    "\n",
    "\n",
    "def get_batch(image, label, image_W, image_H, batch_size, capacity):\n",
    "    '''\n",
    "    Args:\n",
    "        image: list type\n",
    "        label: list type\n",
    "        image_W: image width\n",
    "        image_H: image height\n",
    "        batch_size: batch size\n",
    "        capacity: the maximum elements in queue\n",
    "    Returns:\n",
    "        image_batch: 4D tensor [batch_size, width, height, 3], dtype=tf.float32\n",
    "        label_batch: 1D tensor [batch_size], dtype=tf.int32\n",
    "    '''\n",
    "    \n",
    "    image = tf.cast(image, tf.string)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "\n",
    "    # make an input queue\n",
    "    input_queue = tf.train.slice_input_producer([image, label])\n",
    "    \n",
    "    label = input_queue[1]\n",
    "    image_contents = tf.read_file(input_queue[0])\n",
    "    image = tf.image.decode_jpeg(image_contents, channels=3)\n",
    "    \n",
    "\n",
    "    \n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, image_W, image_H)    \n",
    "\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "    image_batch, label_batch = tf.train.batch([image, label],\n",
    "                                                batch_size= batch_size,\n",
    "                                                num_threads= 4, \n",
    "                                                capacity = capacity)\n",
    "    \n",
    "    label_batch = tf.reshape(label_batch, [batch_size])\n",
    "    image_batch = tf.cast(image_batch, tf.float32)\n",
    "    \n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def inference(images, batch_size, n_classes):\n",
    "    '''Build the model\n",
    "    Args:\n",
    "        images: image batch, 4D tensor, tf.float32, [batch_size, width, height, channels]\n",
    "    Returns:\n",
    "        output tensor with the computed logits, float, [batch_size, n_classes]\n",
    "    '''\n",
    "    #conv1, shape = [kernel size, kernel size, channels, kernel numbers]\n",
    "    \n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        weights = tf.get_variable('weights', \n",
    "                                  shape = [3,3,3, 16],\n",
    "                                  dtype = tf.float32, \n",
    "                                  initializer=tf.initializers.zeros(dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases', \n",
    "                                 shape=[16],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(images, weights, strides=[1,1,1,1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name= scope.name)\n",
    "    \n",
    "    #pool1 and norm1   \n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1],strides=[1,2,2,1],\n",
    "                               padding='SAME', name='pooling1')\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0,\n",
    "                          beta=0.75,name='norm1')\n",
    "    \n",
    "    #conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[3,3,16,16],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[16], \n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(norm1, weights, strides=[1,1,1,1],padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
    "    \n",
    "    \n",
    "    #pool2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0,\n",
    "                          beta=0.75,name='norm2')\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,1,1,1],\n",
    "                               padding='SAME',name='pooling2')\n",
    "    \n",
    "    #local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        reshape = tf.reshape(pool2, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[dim,128],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[128],\n",
    "                                 dtype=tf.float32, \n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)    \n",
    "    \n",
    "    #local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[128,128],\n",
    "                                  dtype=tf.float32, \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[128],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name='local4')\n",
    "     \n",
    "        \n",
    "    # softmax\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = tf.get_variable('softmax_linear',\n",
    "                                  shape=[128, n_classes],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases', \n",
    "                                 shape=[n_classes],\n",
    "                                 dtype=tf.float32, \n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name='softmax_linear')\n",
    "    \n",
    "    return softmax_linear\n",
    "\n",
    "#%%\n",
    "def losses(logits, labels):\n",
    "    '''Compute loss from logits and labels\n",
    "    Args:\n",
    "        logits: logits tensor, float, [batch_size, n_classes]\n",
    "        labels: label tensor, tf.int32, [batch_size]\n",
    "        \n",
    "    Returns:\n",
    "        loss tensor of float type\n",
    "    '''\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        labels = tf.cast(labels,tf.int32)\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits\\\n",
    "                    (logits=logits, labels=labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name+'/loss', loss)\n",
    "    return loss\n",
    "\n",
    "#%%\n",
    "def trainning(loss, learning_rate):\n",
    "    '''Training ops, the Op returned by this function is what must be passed to \n",
    "        'sess.run()' call to cause the model to train.\n",
    "        \n",
    "    Args:\n",
    "        loss: loss tensor, from losses()\n",
    "        \n",
    "    Returns:\n",
    "        train_op: The op for trainning\n",
    "    '''\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step= global_step)\n",
    "    return train_op\n",
    "\n",
    "#%%\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "      range [0, NUM_CLASSES).\n",
    "  Returns:\n",
    "    A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "    that were predicted correctly.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        tf.summary.scalar(scope.name+'/accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "N_CLASSES = 2\n",
    "\n",
    "MAX_STEP = 200 # with current parameters, it is suggested to use MAX_STEP>10k\n",
    "learning_rate = 0.0001 # with current parameters, it is suggested to use learning rate<0.0001\n",
    "\n",
    "logs_train_dir = 'train_log'\n",
    "logs_val_dir = 'test_log'\n",
    "\n",
    "#%%\n",
    "def run_training(origin):\n",
    "    if origin:\n",
    "        IMG_W = 768  # resize the image, if the input image is too large, training will be very slow.\n",
    "        IMG_H = 768\n",
    "        train_dir = 'trainOriginImg'\n",
    "        test_dir = 'testOriginImg'\n",
    "        BATCH_SIZE = 8\n",
    "        CAPACITY = 200\n",
    "    else:\n",
    "        IMG_W = 200  # resize the image, if the input image is too large, training will be very slow.\n",
    "        IMG_H = 200\n",
    "        train_dir = 'trainImg'\n",
    "        test_dir = 'testImg'\n",
    "        BATCH_SIZE = 16\n",
    "        CAPACITY = 2000\n",
    "        \n",
    "    # you need to change the directories to yours.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    train, train_label, val, val_label = get_image_and_labels(train_dir, test_dir,origin)\n",
    "    train_batch, train_label_batch = get_batch(train,\n",
    "                                              train_label,\n",
    "                                              IMG_W,\n",
    "                                              IMG_H,\n",
    "                                              BATCH_SIZE, \n",
    "                                              CAPACITY)\n",
    "    val_batch, val_label_batch = get_batch(val,\n",
    "                                          val_label,\n",
    "                                          IMG_W,\n",
    "                                          IMG_H,\n",
    "                                          BATCH_SIZE, \n",
    "                                          CAPACITY)\n",
    "\n",
    "    \n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "    y_ = tf.placeholder(tf.int32,[BATCH_SIZE])\n",
    "   # y_ = tf.placeholder(tf.int32, shape=[BATCH_SIZE,])\n",
    "    \n",
    "    logits = inference(x, BATCH_SIZE, N_CLASSES)\n",
    "    loss = losses(logits, y_)  \n",
    "    acc = evaluation(logits, y_)\n",
    "    train_op = trainning(loss, learning_rate)\n",
    "    \n",
    "    \n",
    "             \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess= sess, coord=coord)\n",
    "        \n",
    "        summary_op = tf.summary.merge_all()        \n",
    "        train_writer = tf.summary.FileWriter(logs_train_dir, sess.graph)\n",
    "        val_writer = tf.summary.FileWriter(logs_val_dir, sess.graph)\n",
    "    \n",
    "        try:\n",
    "            for step in np.arange(MAX_STEP):\n",
    "                if coord.should_stop():\n",
    "                        break\n",
    "                \n",
    "                tra_images,tra_labels = sess.run([train_batch, train_label_batch])\n",
    "                #tra_labels = tf.cast(tra_labels,tf.int32)\n",
    "          \n",
    "            \n",
    "                _, tra_loss, tra_acc = sess.run([train_op, loss, acc],\n",
    "                                                feed_dict={x:tra_images, y_:tra_labels})\n",
    "                \n",
    "                if step % 20== 0:\n",
    "                    print('Step %d, train loss = %.2f, train accuracy = %.2f%%' %(step, tra_loss, tra_acc*100.0))\n",
    "                    summary_str = sess.run(summary_op,feed_dict={x:tra_images, y_:tra_labels})\n",
    "                    train_writer.add_summary(summary_str, step)\n",
    "                    \n",
    "                 \n",
    "                if step % 50 == 0 or (step + 1) == MAX_STEP:\n",
    "                    val_images, val_labels = sess.run([val_batch, val_label_batch])\n",
    "                    \n",
    "                    \n",
    "                    val_loss, val_acc = sess.run([loss, acc], \n",
    "                                                 feed_dict={x:val_images, y_:val_labels})\n",
    "                    print('**  Step %d, val loss = %.2f, val accuracy = %.2f%%  **' %(step, val_loss, val_acc*100.0))\n",
    "                    summary_str = sess.run(summary_op,feed_dict={x:val_images, y_:val_labels}\n",
    "                    )\n",
    "                    val_writer.add_summary(summary_str, step)  \n",
    "                                    \n",
    "                if step % 100 == 0 or (step + 1) == MAX_STEP:\n",
    "                    checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path, global_step=step)\n",
    "                    \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            coord.request_stop()           \n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearDir(path):\n",
    "    if os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "clearDir(logs_train_dir)\n",
    "clearDir(logs_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-8700d470a3fd>:69: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:372: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:318: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-1-8700d470a3fd>:84: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-3-95ad4ea558a3>:67: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Step 0, train loss = 0.69, train accuracy = 87.50%\n",
      "**  Step 0, val loss = 0.64, val accuracy = 75.00%  **\n",
      "Step 20, train loss = 0.96, train accuracy = 50.00%\n",
      "Step 40, train loss = 0.38, train accuracy = 87.50%\n",
      "**  Step 50, val loss = 0.25, val accuracy = 100.00%  **\n",
      "Step 60, train loss = 0.19, train accuracy = 100.00%\n",
      "Step 80, train loss = 0.60, train accuracy = 75.00%\n",
      "Step 100, train loss = 0.12, train accuracy = 100.00%\n",
      "**  Step 100, val loss = 0.10, val accuracy = 100.00%  **\n",
      "Step 120, train loss = 1.07, train accuracy = 50.00%\n",
      "Step 140, train loss = 0.50, train accuracy = 75.00%\n",
      "**  Step 150, val loss = 0.87, val accuracy = 62.50%  **\n",
      "Step 160, train loss = 0.27, train accuracy = 100.00%\n",
      "Step 180, train loss = 0.59, train accuracy = 62.50%\n",
      "**  Step 199, val loss = 0.64, val accuracy = 62.50%  **\n"
     ]
    }
   ],
   "source": [
    "run_training(origin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-8700d470a3fd>:69: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:372: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:318: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-1-8700d470a3fd>:84: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-3-95ad4ea558a3>:67: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Step 0, train loss = 0.70, train accuracy = 25.00%\n",
      "**  Step 0, val loss = 0.69, val accuracy = 56.25%  **\n",
      "Step 20, train loss = 0.57, train accuracy = 75.00%\n",
      "Step 40, train loss = 0.74, train accuracy = 56.25%\n",
      "**  Step 50, val loss = 0.72, val accuracy = 56.25%  **\n",
      "Step 60, train loss = 0.47, train accuracy = 81.25%\n",
      "Step 80, train loss = 0.61, train accuracy = 68.75%\n",
      "Step 100, train loss = 0.51, train accuracy = 81.25%\n",
      "**  Step 100, val loss = 0.49, val accuracy = 81.25%  **\n",
      "Step 120, train loss = 0.51, train accuracy = 81.25%\n",
      "Step 140, train loss = 0.41, train accuracy = 87.50%\n",
      "**  Step 150, val loss = 0.55, val accuracy = 68.75%  **\n",
      "Step 160, train loss = 0.72, train accuracy = 62.50%\n",
      "Step 180, train loss = 0.72, train accuracy = 62.50%\n",
      "**  Step 199, val loss = 0.29, val accuracy = 87.50%  **\n"
     ]
    }
   ],
   "source": [
    "run_training(origin=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 Nik Bear Brown\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
