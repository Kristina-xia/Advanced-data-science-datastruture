{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "### Basic Preparation\n",
    "#### Importing packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from collections import defaultdict\n",
    "from sklearn import svm # SVM classifier\n",
    "from sklearn.neural_network import MLPClassifier # MLP classifier\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,KFold,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve # ROC Curves\n",
    "from sklearn.metrics import auc # Calculating AUC for ROC's!\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import model_selection\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run</th>\n",
       "      <th>rerun</th>\n",
       "      <th>camcol</th>\n",
       "      <th>field</th>\n",
       "      <th>specobjid</th>\n",
       "      <th>class</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.531326</td>\n",
       "      <td>0.089693</td>\n",
       "      <td>19.47406</td>\n",
       "      <td>17.04240</td>\n",
       "      <td>15.94699</td>\n",
       "      <td>15.50342</td>\n",
       "      <td>15.22531</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.722360e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.598371</td>\n",
       "      <td>0.135285</td>\n",
       "      <td>18.66280</td>\n",
       "      <td>17.21449</td>\n",
       "      <td>16.67637</td>\n",
       "      <td>16.48922</td>\n",
       "      <td>16.39150</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.638140e+17</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>323</td>\n",
       "      <td>51615</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.680207</td>\n",
       "      <td>0.126185</td>\n",
       "      <td>19.38298</td>\n",
       "      <td>18.19169</td>\n",
       "      <td>17.47428</td>\n",
       "      <td>17.08732</td>\n",
       "      <td>16.80125</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>268</td>\n",
       "      <td>3.232740e+17</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>287</td>\n",
       "      <td>52023</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.870529</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>17.76536</td>\n",
       "      <td>16.60272</td>\n",
       "      <td>16.16116</td>\n",
       "      <td>15.98233</td>\n",
       "      <td>15.90438</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.883288</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>17.55025</td>\n",
       "      <td>16.26342</td>\n",
       "      <td>16.43869</td>\n",
       "      <td>16.55492</td>\n",
       "      <td>16.61326</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          objid          ra       dec         u         g         r         i  \\\n",
       "0  1.237650e+18  183.531326  0.089693  19.47406  17.04240  15.94699  15.50342   \n",
       "1  1.237650e+18  183.598371  0.135285  18.66280  17.21449  16.67637  16.48922   \n",
       "2  1.237650e+18  183.680207  0.126185  19.38298  18.19169  17.47428  17.08732   \n",
       "3  1.237650e+18  183.870529  0.049911  17.76536  16.60272  16.16116  15.98233   \n",
       "4  1.237650e+18  183.883288  0.102557  17.55025  16.26342  16.43869  16.55492   \n",
       "\n",
       "          z  run  rerun  camcol  field     specobjid   class  redshift  plate  \\\n",
       "0  15.22531  752    301       4    267  3.722360e+18    STAR -0.000009   3306   \n",
       "1  16.39150  752    301       4    267  3.638140e+17    STAR -0.000055    323   \n",
       "2  16.80125  752    301       4    268  3.232740e+17  GALAXY  0.123111    287   \n",
       "3  15.90438  752    301       4    269  3.722370e+18    STAR -0.000111   3306   \n",
       "4  16.61326  752    301       4    269  3.722370e+18    STAR  0.000590   3306   \n",
       "\n",
       "     mjd  fiberid  \n",
       "0  54922      491  \n",
       "1  51615      541  \n",
       "2  52023      513  \n",
       "3  54922      510  \n",
       "4  54922      512  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import original data\n",
    "\n",
    "ss= pd.read_csv(\"skyserver.csv\")\n",
    "ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 18 columns):\n",
      "objid        10000 non-null float64\n",
      "ra           10000 non-null float64\n",
      "dec          10000 non-null float64\n",
      "u            10000 non-null float64\n",
      "g            10000 non-null float64\n",
      "r            10000 non-null float64\n",
      "i            10000 non-null float64\n",
      "z            10000 non-null float64\n",
      "run          10000 non-null int64\n",
      "rerun        10000 non-null int64\n",
      "camcol       10000 non-null int64\n",
      "field        10000 non-null int64\n",
      "specobjid    10000 non-null float64\n",
      "class        10000 non-null object\n",
      "redshift     10000 non-null float64\n",
      "plate        10000 non-null int64\n",
      "mjd          10000 non-null int64\n",
      "fiberid      10000 non-null int64\n",
      "dtypes: float64(10), int64(7), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "ss.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I want to research on is 'class' column, which is the dependent value. So here I moved it to the far left of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>objid</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run</th>\n",
       "      <th>rerun</th>\n",
       "      <th>camcol</th>\n",
       "      <th>field</th>\n",
       "      <th>specobjid</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STAR</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.531326</td>\n",
       "      <td>0.089693</td>\n",
       "      <td>19.47406</td>\n",
       "      <td>17.04240</td>\n",
       "      <td>15.94699</td>\n",
       "      <td>15.50342</td>\n",
       "      <td>15.22531</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.722360e+18</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STAR</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.598371</td>\n",
       "      <td>0.135285</td>\n",
       "      <td>18.66280</td>\n",
       "      <td>17.21449</td>\n",
       "      <td>16.67637</td>\n",
       "      <td>16.48922</td>\n",
       "      <td>16.39150</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.638140e+17</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>323</td>\n",
       "      <td>51615</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GALAXY</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.680207</td>\n",
       "      <td>0.126185</td>\n",
       "      <td>19.38298</td>\n",
       "      <td>18.19169</td>\n",
       "      <td>17.47428</td>\n",
       "      <td>17.08732</td>\n",
       "      <td>16.80125</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>268</td>\n",
       "      <td>3.232740e+17</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>287</td>\n",
       "      <td>52023</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STAR</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.870529</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>17.76536</td>\n",
       "      <td>16.60272</td>\n",
       "      <td>16.16116</td>\n",
       "      <td>15.98233</td>\n",
       "      <td>15.90438</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STAR</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.883288</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>17.55025</td>\n",
       "      <td>16.26342</td>\n",
       "      <td>16.43869</td>\n",
       "      <td>16.55492</td>\n",
       "      <td>16.61326</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class         objid          ra       dec         u         g         r  \\\n",
       "0    STAR  1.237650e+18  183.531326  0.089693  19.47406  17.04240  15.94699   \n",
       "1    STAR  1.237650e+18  183.598371  0.135285  18.66280  17.21449  16.67637   \n",
       "2  GALAXY  1.237650e+18  183.680207  0.126185  19.38298  18.19169  17.47428   \n",
       "3    STAR  1.237650e+18  183.870529  0.049911  17.76536  16.60272  16.16116   \n",
       "4    STAR  1.237650e+18  183.883288  0.102557  17.55025  16.26342  16.43869   \n",
       "\n",
       "          i         z  run  rerun  camcol  field     specobjid  redshift  \\\n",
       "0  15.50342  15.22531  752    301       4    267  3.722360e+18 -0.000009   \n",
       "1  16.48922  16.39150  752    301       4    267  3.638140e+17 -0.000055   \n",
       "2  17.08732  16.80125  752    301       4    268  3.232740e+17  0.123111   \n",
       "3  15.98233  15.90438  752    301       4    269  3.722370e+18 -0.000111   \n",
       "4  16.55492  16.61326  752    301       4    269  3.722370e+18  0.000590   \n",
       "\n",
       "   plate    mjd  fiberid  \n",
       "0   3306  54922      491  \n",
       "1    323  51615      541  \n",
       "2    287  52023      513  \n",
       "3   3306  54922      510  \n",
       "4   3306  54922      512  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl=ss['class']\n",
    "ss.drop(labels=['class'], axis=1,inplace = True)\n",
    "ss.insert(0, 'class', cl)\n",
    "ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run</th>\n",
       "      <th>rerun</th>\n",
       "      <th>camcol</th>\n",
       "      <th>field</th>\n",
       "      <th>specobjid</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>175.529987</td>\n",
       "      <td>14.836148</td>\n",
       "      <td>18.619355</td>\n",
       "      <td>17.371931</td>\n",
       "      <td>16.840963</td>\n",
       "      <td>16.583579</td>\n",
       "      <td>16.422833</td>\n",
       "      <td>981.034800</td>\n",
       "      <td>301.0</td>\n",
       "      <td>3.648700</td>\n",
       "      <td>302.380100</td>\n",
       "      <td>1.645022e+18</td>\n",
       "      <td>0.143726</td>\n",
       "      <td>1460.986400</td>\n",
       "      <td>52943.533300</td>\n",
       "      <td>353.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.577039e+05</td>\n",
       "      <td>47.783439</td>\n",
       "      <td>25.212207</td>\n",
       "      <td>0.828656</td>\n",
       "      <td>0.945457</td>\n",
       "      <td>1.067764</td>\n",
       "      <td>1.141805</td>\n",
       "      <td>1.203188</td>\n",
       "      <td>273.305024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666183</td>\n",
       "      <td>162.577763</td>\n",
       "      <td>2.013998e+18</td>\n",
       "      <td>0.388774</td>\n",
       "      <td>1788.778371</td>\n",
       "      <td>1511.150651</td>\n",
       "      <td>206.298149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>8.235100</td>\n",
       "      <td>-5.382632</td>\n",
       "      <td>12.988970</td>\n",
       "      <td>12.799550</td>\n",
       "      <td>12.431600</td>\n",
       "      <td>11.947210</td>\n",
       "      <td>11.610410</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.995780e+17</td>\n",
       "      <td>-0.004136</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>51578.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>157.370946</td>\n",
       "      <td>-0.539035</td>\n",
       "      <td>18.178035</td>\n",
       "      <td>16.815100</td>\n",
       "      <td>16.173333</td>\n",
       "      <td>15.853705</td>\n",
       "      <td>15.618285</td>\n",
       "      <td>752.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>3.389248e+17</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>51900.000000</td>\n",
       "      <td>186.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>180.394514</td>\n",
       "      <td>0.404166</td>\n",
       "      <td>18.853095</td>\n",
       "      <td>17.495135</td>\n",
       "      <td>16.858770</td>\n",
       "      <td>16.554985</td>\n",
       "      <td>16.389945</td>\n",
       "      <td>756.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>4.966580e+17</td>\n",
       "      <td>0.042591</td>\n",
       "      <td>441.000000</td>\n",
       "      <td>51997.000000</td>\n",
       "      <td>351.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>201.547279</td>\n",
       "      <td>35.649397</td>\n",
       "      <td>19.259232</td>\n",
       "      <td>18.010145</td>\n",
       "      <td>17.512675</td>\n",
       "      <td>17.258550</td>\n",
       "      <td>17.141447</td>\n",
       "      <td>1331.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>2.881300e+18</td>\n",
       "      <td>0.092579</td>\n",
       "      <td>2559.000000</td>\n",
       "      <td>54468.000000</td>\n",
       "      <td>510.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>260.884382</td>\n",
       "      <td>68.542265</td>\n",
       "      <td>19.599900</td>\n",
       "      <td>19.918970</td>\n",
       "      <td>24.802040</td>\n",
       "      <td>28.179630</td>\n",
       "      <td>22.833060</td>\n",
       "      <td>1412.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>9.468830e+18</td>\n",
       "      <td>5.353854</td>\n",
       "      <td>8410.000000</td>\n",
       "      <td>57481.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              objid            ra           dec             u             g  \\\n",
       "count  1.000000e+04  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean   1.237650e+18    175.529987     14.836148     18.619355     17.371931   \n",
       "std    1.577039e+05     47.783439     25.212207      0.828656      0.945457   \n",
       "min    1.237650e+18      8.235100     -5.382632     12.988970     12.799550   \n",
       "25%    1.237650e+18    157.370946     -0.539035     18.178035     16.815100   \n",
       "50%    1.237650e+18    180.394514      0.404166     18.853095     17.495135   \n",
       "75%    1.237650e+18    201.547279     35.649397     19.259232     18.010145   \n",
       "max    1.237650e+18    260.884382     68.542265     19.599900     19.918970   \n",
       "\n",
       "                  r             i             z           run    rerun  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.0   \n",
       "mean      16.840963     16.583579     16.422833    981.034800    301.0   \n",
       "std        1.067764      1.141805      1.203188    273.305024      0.0   \n",
       "min       12.431600     11.947210     11.610410    308.000000    301.0   \n",
       "25%       16.173333     15.853705     15.618285    752.000000    301.0   \n",
       "50%       16.858770     16.554985     16.389945    756.000000    301.0   \n",
       "75%       17.512675     17.258550     17.141447   1331.000000    301.0   \n",
       "max       24.802040     28.179630     22.833060   1412.000000    301.0   \n",
       "\n",
       "             camcol         field     specobjid      redshift         plate  \\\n",
       "count  10000.000000  10000.000000  1.000000e+04  10000.000000  10000.000000   \n",
       "mean       3.648700    302.380100  1.645022e+18      0.143726   1460.986400   \n",
       "std        1.666183    162.577763  2.013998e+18      0.388774   1788.778371   \n",
       "min        1.000000     11.000000  2.995780e+17     -0.004136    266.000000   \n",
       "25%        2.000000    184.000000  3.389248e+17      0.000081    301.000000   \n",
       "50%        4.000000    299.000000  4.966580e+17      0.042591    441.000000   \n",
       "75%        5.000000    414.000000  2.881300e+18      0.092579   2559.000000   \n",
       "max        6.000000    768.000000  9.468830e+18      5.353854   8410.000000   \n",
       "\n",
       "                mjd       fiberid  \n",
       "count  10000.000000  10000.000000  \n",
       "mean   52943.533300    353.069400  \n",
       "std     1511.150651    206.298149  \n",
       "min    51578.000000      1.000000  \n",
       "25%    51900.000000    186.750000  \n",
       "50%    51997.000000    351.000000  \n",
       "75%    54468.000000    510.000000  \n",
       "max    57481.000000   1000.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning data\n",
    "\n",
    "##### Brief Introduction about each column\n",
    "\n",
    "objid = Object Identifier\n",
    "\n",
    "ra = J2000 Right Ascension (r-band)-\n",
    "    \n",
    "    (The coordinates on the sky that correspond to longitude on Earth. Ra measures east and west on the celestial sphere and is like longitude on the Earth.)\n",
    "\n",
    "dec = J2000 Declination (r-band)\n",
    "\n",
    "    (The coordinates on the sky that correspond to latitude on Earth. Dec measures north and south on the celestial sphere and is like latitude on the Earth.)\n",
    "    \n",
    "u (ultraviolet)= better of DeV/Exp magnitude fit\n",
    "\n",
    "g (green) = better of DeV/Exp magnitude fit\n",
    "\n",
    "r (red) = better of DeV/Exp magnitude fit\n",
    "\n",
    "i (Near infrared) = better of DeV/Exp magnitude fit\n",
    "\n",
    "z (Infrared) = better of DeV/Exp magnitude fit\n",
    "\n",
    "run = Run Number\n",
    "\n",
    "rereun = Rerun Number\n",
    "\n",
    "camcol = Camera column\n",
    "\n",
    "field = Field number\n",
    "\n",
    "specobjid = Object Identifier\n",
    "\n",
    "class = object class (galaxy, star or quasar object)\n",
    "\n",
    "redshift = Final Redshift\n",
    "\n",
    "plate = plate number\n",
    "\n",
    "mjd = MJD of observation\n",
    "\n",
    "fiberid = fiber ID\n",
    "\n",
    "From the definition of each columns, it's obvious that objid, specobjid and fiberid are useless to the prediction to  the class. So I chose to drop the three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.drop(['objid','specobjid','fiberid','run','rerun','camcol','field'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      "class       10000 non-null object\n",
      "ra          10000 non-null float64\n",
      "dec         10000 non-null float64\n",
      "u           10000 non-null float64\n",
      "g           10000 non-null float64\n",
      "r           10000 non-null float64\n",
      "i           10000 non-null float64\n",
      "z           10000 non-null float64\n",
      "redshift    10000 non-null float64\n",
      "plate       10000 non-null int64\n",
      "mjd         10000 non-null int64\n",
      "dtypes: float64(8), int64(2), object(1)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "ss.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STAR</td>\n",
       "      <td>183.531326</td>\n",
       "      <td>0.089693</td>\n",
       "      <td>19.47406</td>\n",
       "      <td>17.04240</td>\n",
       "      <td>15.94699</td>\n",
       "      <td>15.50342</td>\n",
       "      <td>15.22531</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STAR</td>\n",
       "      <td>183.598371</td>\n",
       "      <td>0.135285</td>\n",
       "      <td>18.66280</td>\n",
       "      <td>17.21449</td>\n",
       "      <td>16.67637</td>\n",
       "      <td>16.48922</td>\n",
       "      <td>16.39150</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>323</td>\n",
       "      <td>51615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GALAXY</td>\n",
       "      <td>183.680207</td>\n",
       "      <td>0.126185</td>\n",
       "      <td>19.38298</td>\n",
       "      <td>18.19169</td>\n",
       "      <td>17.47428</td>\n",
       "      <td>17.08732</td>\n",
       "      <td>16.80125</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>287</td>\n",
       "      <td>52023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STAR</td>\n",
       "      <td>183.870529</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>17.76536</td>\n",
       "      <td>16.60272</td>\n",
       "      <td>16.16116</td>\n",
       "      <td>15.98233</td>\n",
       "      <td>15.90438</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STAR</td>\n",
       "      <td>183.883288</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>17.55025</td>\n",
       "      <td>16.26342</td>\n",
       "      <td>16.43869</td>\n",
       "      <td>16.55492</td>\n",
       "      <td>16.61326</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class          ra       dec         u         g         r         i  \\\n",
       "0    STAR  183.531326  0.089693  19.47406  17.04240  15.94699  15.50342   \n",
       "1    STAR  183.598371  0.135285  18.66280  17.21449  16.67637  16.48922   \n",
       "2  GALAXY  183.680207  0.126185  19.38298  18.19169  17.47428  17.08732   \n",
       "3    STAR  183.870529  0.049911  17.76536  16.60272  16.16116  15.98233   \n",
       "4    STAR  183.883288  0.102557  17.55025  16.26342  16.43869  16.55492   \n",
       "\n",
       "          z  redshift  plate    mjd  \n",
       "0  15.22531 -0.000009   3306  54922  \n",
       "1  16.39150 -0.000055    323  51615  \n",
       "2  16.80125  0.123111    287  52023  \n",
       "3  15.90438 -0.000111   3306  54922  \n",
       "4  16.61326  0.000590   3306  54922  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label Encoding\n",
    "\n",
    "On the ground that the datatype of 'class',the dependent value is object. In order to do later research, so I used label encoding here for the 'class' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=ss.iloc[:,1:]\n",
    "y=ss.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=le.fit_transform(y)\n",
    "y=pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>183.531326</td>\n",
       "      <td>0.089693</td>\n",
       "      <td>19.47406</td>\n",
       "      <td>17.04240</td>\n",
       "      <td>15.94699</td>\n",
       "      <td>15.50342</td>\n",
       "      <td>15.22531</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>183.598371</td>\n",
       "      <td>0.135285</td>\n",
       "      <td>18.66280</td>\n",
       "      <td>17.21449</td>\n",
       "      <td>16.67637</td>\n",
       "      <td>16.48922</td>\n",
       "      <td>16.39150</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>323</td>\n",
       "      <td>51615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>183.680207</td>\n",
       "      <td>0.126185</td>\n",
       "      <td>19.38298</td>\n",
       "      <td>18.19169</td>\n",
       "      <td>17.47428</td>\n",
       "      <td>17.08732</td>\n",
       "      <td>16.80125</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>287</td>\n",
       "      <td>52023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>183.870529</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>17.76536</td>\n",
       "      <td>16.60272</td>\n",
       "      <td>16.16116</td>\n",
       "      <td>15.98233</td>\n",
       "      <td>15.90438</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>183.883288</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>17.55025</td>\n",
       "      <td>16.26342</td>\n",
       "      <td>16.43869</td>\n",
       "      <td>16.55492</td>\n",
       "      <td>16.61326</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class          ra       dec         u         g         r         i  \\\n",
       "0      2  183.531326  0.089693  19.47406  17.04240  15.94699  15.50342   \n",
       "1      2  183.598371  0.135285  18.66280  17.21449  16.67637  16.48922   \n",
       "2      0  183.680207  0.126185  19.38298  18.19169  17.47428  17.08732   \n",
       "3      2  183.870529  0.049911  17.76536  16.60272  16.16116  15.98233   \n",
       "4      2  183.883288  0.102557  17.55025  16.26342  16.43869  16.55492   \n",
       "\n",
       "          z  redshift  plate    mjd  \n",
       "0  15.22531 -0.000009   3306  54922  \n",
       "1  16.39150 -0.000055    323  51615  \n",
       "2  16.80125  0.123111    287  52023  \n",
       "3  15.90438 -0.000111   3306  54922  \n",
       "4  16.61326  0.000590   3306  54922  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss=pd.concat([y,X],axis=1,sort=False)\n",
    "ss.rename(index=str, columns={0: \"class\"},inplace=True)\n",
    "ss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### detecting empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class       0\n",
       "ra          0\n",
       "dec         0\n",
       "u           0\n",
       "g           0\n",
       "r           0\n",
       "i           0\n",
       "z           0\n",
       "redshift    0\n",
       "plate       0\n",
       "mjd         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, there is no empty value.\n",
    "\n",
    "##### Preprocessing: Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_df(df):\n",
    "    scaled = pd.DataFrame()\n",
    "    for item in df:\n",
    "        if item in df.select_dtypes(include=[np.float]):\n",
    "            scaled[item] = ((df[item] - df[item].min()) / \n",
    "            (df[item].max() - df[item].min()))\n",
    "        else: \n",
    "            scaled[item] = df[item]\n",
    "    return scaled\n",
    "ss_scaled= scaled_df(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.693832</td>\n",
       "      <td>0.074025</td>\n",
       "      <td>0.980965</td>\n",
       "      <td>0.595954</td>\n",
       "      <td>0.284177</td>\n",
       "      <td>0.219081</td>\n",
       "      <td>0.322108</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.694098</td>\n",
       "      <td>0.074642</td>\n",
       "      <td>0.858250</td>\n",
       "      <td>0.620126</td>\n",
       "      <td>0.343138</td>\n",
       "      <td>0.279811</td>\n",
       "      <td>0.426021</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>323</td>\n",
       "      <td>51615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.694422</td>\n",
       "      <td>0.074519</td>\n",
       "      <td>0.967188</td>\n",
       "      <td>0.757385</td>\n",
       "      <td>0.407640</td>\n",
       "      <td>0.316657</td>\n",
       "      <td>0.462532</td>\n",
       "      <td>0.023749</td>\n",
       "      <td>287</td>\n",
       "      <td>52023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.695175</td>\n",
       "      <td>0.073487</td>\n",
       "      <td>0.722499</td>\n",
       "      <td>0.534197</td>\n",
       "      <td>0.301490</td>\n",
       "      <td>0.248584</td>\n",
       "      <td>0.382616</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.695225</td>\n",
       "      <td>0.074199</td>\n",
       "      <td>0.689960</td>\n",
       "      <td>0.486538</td>\n",
       "      <td>0.323925</td>\n",
       "      <td>0.283858</td>\n",
       "      <td>0.445782</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class        ra       dec         u         g         r         i  \\\n",
       "0      2  0.693832  0.074025  0.980965  0.595954  0.284177  0.219081   \n",
       "1      2  0.694098  0.074642  0.858250  0.620126  0.343138  0.279811   \n",
       "2      0  0.694422  0.074519  0.967188  0.757385  0.407640  0.316657   \n",
       "3      2  0.695175  0.073487  0.722499  0.534197  0.301490  0.248584   \n",
       "4      2  0.695225  0.074199  0.689960  0.486538  0.323925  0.283858   \n",
       "\n",
       "          z  redshift  plate    mjd  \n",
       "0  0.322108  0.000770   3306  54922  \n",
       "1  0.426021  0.000762    323  51615  \n",
       "2  0.462532  0.023749    287  52023  \n",
       "3  0.382616  0.000751   3306  54922  \n",
       "4  0.445782  0.000882   3306  54922  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.915400</td>\n",
       "      <td>0.662163</td>\n",
       "      <td>0.273504</td>\n",
       "      <td>0.851678</td>\n",
       "      <td>0.642241</td>\n",
       "      <td>0.356444</td>\n",
       "      <td>0.285624</td>\n",
       "      <td>0.428813</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>1460.986400</td>\n",
       "      <td>52943.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.952856</td>\n",
       "      <td>0.189130</td>\n",
       "      <td>0.341052</td>\n",
       "      <td>0.125346</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>0.086316</td>\n",
       "      <td>0.070341</td>\n",
       "      <td>0.107211</td>\n",
       "      <td>0.072560</td>\n",
       "      <td>1788.778371</td>\n",
       "      <td>1511.150651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>51578.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590288</td>\n",
       "      <td>0.065521</td>\n",
       "      <td>0.784922</td>\n",
       "      <td>0.564028</td>\n",
       "      <td>0.302474</td>\n",
       "      <td>0.240660</td>\n",
       "      <td>0.357124</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>51900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681417</td>\n",
       "      <td>0.078279</td>\n",
       "      <td>0.887035</td>\n",
       "      <td>0.659546</td>\n",
       "      <td>0.357883</td>\n",
       "      <td>0.283862</td>\n",
       "      <td>0.425883</td>\n",
       "      <td>0.008721</td>\n",
       "      <td>441.000000</td>\n",
       "      <td>51997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.765140</td>\n",
       "      <td>0.555050</td>\n",
       "      <td>0.948469</td>\n",
       "      <td>0.731885</td>\n",
       "      <td>0.410743</td>\n",
       "      <td>0.327206</td>\n",
       "      <td>0.492846</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>2559.000000</td>\n",
       "      <td>54468.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8410.000000</td>\n",
       "      <td>57481.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              class            ra           dec             u             g  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.915400      0.662163      0.273504      0.851678      0.642241   \n",
       "std        0.952856      0.189130      0.341052      0.125346      0.132800   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.590288      0.065521      0.784922      0.564028   \n",
       "50%        1.000000      0.681417      0.078279      0.887035      0.659546   \n",
       "75%        2.000000      0.765140      0.555050      0.948469      0.731885   \n",
       "max        2.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                  r             i             z      redshift         plate  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.356444      0.285624      0.428813      0.027597   1460.986400   \n",
       "std        0.086316      0.070341      0.107211      0.072560   1788.778371   \n",
       "min        0.000000      0.000000      0.000000      0.000000    266.000000   \n",
       "25%        0.302474      0.240660      0.357124      0.000787    301.000000   \n",
       "50%        0.357883      0.283862      0.425883      0.008721    441.000000   \n",
       "75%        0.410743      0.327206      0.492846      0.018051   2559.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000   8410.000000   \n",
       "\n",
       "                mjd  \n",
       "count  10000.000000  \n",
       "mean   52943.533300  \n",
       "std     1511.150651  \n",
       "min    51578.000000  \n",
       "25%    51900.000000  \n",
       "50%    51997.000000  \n",
       "75%    54468.000000  \n",
       "max    57481.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using zscore to detect outlier and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.13831973, 0.16745842, 0.58492272, ..., 0.36973112, 1.03148936,\n",
       "        1.30931064],\n",
       "       [1.13831973, 0.16886159, 0.58311429, ..., 0.36984929, 0.63621258,\n",
       "        0.87919741],\n",
       "       [0.96073933, 0.17057433, 0.58347525, ..., 0.05302706, 0.65633905,\n",
       "        0.60919097],\n",
       "       ...,\n",
       "       [1.13831973, 0.92039468, 1.46090664, ..., 0.37074104, 3.26608697,\n",
       "        2.69309363],\n",
       "       [0.96073933, 0.92197294, 1.46432109, ..., 0.33364691, 0.56688806,\n",
       "        0.70581092],\n",
       "       [0.96073933, 0.91804124, 1.46639319, ..., 0.06510125, 0.56688806,\n",
       "        0.70581092]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=np.abs(stats.zscore(ss_scaled))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  45,  109,  129, ..., 9992, 9994, 9997]), array([8, 8, 8, ..., 9, 9, 9]))\n"
     ]
    }
   ],
   "source": [
    "print(np.where(z>3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_scaled=ss_scaled[(z<3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8713 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      "class       8713 non-null int64\n",
      "ra          8713 non-null float64\n",
      "dec         8713 non-null float64\n",
      "u           8713 non-null float64\n",
      "g           8713 non-null float64\n",
      "r           8713 non-null float64\n",
      "i           8713 non-null float64\n",
      "z           8713 non-null float64\n",
      "redshift    8713 non-null float64\n",
      "plate       8713 non-null int64\n",
      "mjd         8713 non-null int64\n",
      "dtypes: float64(8), int64(3)\n",
      "memory usage: 816.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ss_scaled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=ss_scaled.iloc[:,1:]\n",
    "y=ss_scaled[['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6534, 10)\n",
      "(2179, 10)\n",
      "(6534, 1)\n",
      "(2179, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunedParamDtm={'random_state':[10,20,30,40,50,60,70,80,90,100],'criterion':['gini','entropy'],'max_features':[2,3,4,5,6,7,8,9,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmTuned=GridSearchCV(dtm,tunedParamDtm,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'random_state': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 'criterion': ['gini', 'entropy'], 'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtmTuned.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_features': 8, 'random_state': 20}\n"
     ]
    }
   ],
   "source": [
    "print(dtmTuned.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9882154882154882"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtmTuned.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KfoldCrossVal mean score using decision tree is 0.9816404096967878\n",
      "Accuracy score using Decision Tree is 0.9857732905002294\n"
     ]
    }
   ],
   "source": [
    "dtmN=DecisionTreeClassifier(criterion='entropy',max_features=8,random_state=20)\n",
    "\n",
    "print(\"KfoldCrossVal mean score using decision tree is %s\" %cross_val_score(dtmN,X,y,cv=10).mean())\n",
    "\n",
    "\n",
    "sm= dtmN.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sm.predict(X_test)\n",
    "print(\"Accuracy score using Decision Tree is %s\" %metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which hyper-parameters are important?\n",
    "\n",
    "For Decision tree, there are some important hyper-parameters: random_state, criterion, max_features.\n",
    "\n",
    "criterion : The function to measure the quality of a split. Supported criteria are gini for the Gini impurity and entropy for the information gain. Note: this parameter is tree-specific.\n",
    "\n",
    "max_features : The number of features to consider when looking for the best split.\n",
    "\n",
    "random_state : If int, random_state is the seed used by the random number generator.\n",
    "\n",
    "\n",
    "* What hyper-parameter values work best? \n",
    "\n",
    "Through grid search, the best values for decision tree of criterion is entropy and max_features is 8 and random_state is 20.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunedParamRdf={'random_state':[10,20,30,40,50],'criterion':['gini','entropy'],'n_estimators':[20,30,40,50,60,70],'max_features':[2,3,4,5,6,7,8,9,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfTuned=RandomizedSearchCV(rdf,tunedParamRdf,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'random_state': [10, 20, 30, 40, 50], 'criterion': ['gini', 'entropy'], 'n_estimators': [20, 30, 40, 50, 60, 70], 'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdfTuned.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_state': 50, 'n_estimators': 60, 'max_features': 8, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "print(rdfTuned.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905111723293542"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdfTuned.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KfoldCrossVal mean score using random forest is 0.9892093628931932\n",
      "Accuracy score using Random Forest is 0.9880679210647085\n"
     ]
    }
   ],
   "source": [
    "rdfN=RandomForestClassifier(criterion='entropy',max_features=7,n_estimators=30,random_state=40)\n",
    "\n",
    "print(\"KfoldCrossVal mean score using random forest is %s\" %cross_val_score(rdfN,X,y,cv=10).mean())\n",
    "\n",
    "\n",
    "sm= rdfN.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sm.predict(X_test)\n",
    "print(\"Accuracy score using Random Forest is %s\" %metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which hyper-parameters are important?\n",
    "\n",
    "For random forest, there are some important hyper-parameters: random_state, criterion, max_features, n_estimators.\n",
    "\n",
    "criterion : The function to measure the quality of a split. Supported criteria are gini for the Gini impurity and entropy for the information gain. Note: this parameter is tree-specific.\n",
    "\n",
    "max_features : The number of features to consider when looking for the best split.\n",
    "\n",
    "random_state : If int, random_state is the seed used by the random number generator.\n",
    "\n",
    "n_estimators : The number of trees in the forest.\n",
    "\n",
    "* What hyper-parameter values work best? \n",
    "\n",
    "Through grid search, the best values for random forest of criterion is entropy and max_features is 7 and random_state is 40, n_estimators is 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors\n",
    "\n",
    "Cause there is only one important hyper-parameter, here I use a loop to find the best k for K-Nearest Neighbors algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal K is 37\n"
     ]
    }
   ],
   "source": [
    "knn_k=[]\n",
    "for i in range(0,55):\n",
    "    #The kNN classification problem is to find the k nearest data points in a data set to a given query data point. \n",
    "    #The point is then assigned to the group by a majority \"vote.\" \n",
    "    #For this reason, pick an odd k is prefered as the odd vote can break ties. \n",
    "    if(i%2 != 0):\n",
    "        knn_k.append(i)\n",
    "        \n",
    "cross_vals=[]\n",
    "for k in knn_k:\n",
    "    knn=KNeighborsClassifier(n_neighbors=k)\n",
    "    scores=cross_val_score(knn,X_train,y_train,cv=10,scoring='accuracy')\n",
    "    cross_vals.append(scores.mean())\n",
    "\n",
    "optimal_k=knn_k[cross_vals.index(min(cross_vals))]\n",
    "print(\"Optimal K is {0}\".format(optimal_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KfoldCrossVal mean score using K-Nearest Neighbors is 0.8325530738130414\n",
      "Accuracy score using K-Nearest Neighbors is 0.833409821018816\n"
     ]
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=37)\n",
    "\n",
    "print(\"KfoldCrossVal mean score using K-Nearest Neighbors is %s\" %cross_val_score(knn,X,y,cv=10).mean())\n",
    "\n",
    "\n",
    "sm= knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sm.predict(X_test)\n",
    "print(\"Accuracy score using K-Nearest Neighbors is %s\" %metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which hyper-parameters are important?\n",
    "\n",
    "For K-Nearest Neighbors algorithm, k matters.\n",
    "\n",
    "K: Number of neighbors to use by default for kneighbors queries.\n",
    "\n",
    "* What hyper-parameter values work best? \n",
    "\n",
    "Through grid search, the best values of k is 37."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "Here using RandomizedSearchCV to find the best values of hyper-parameters for naive bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TunedParamMnb={'alpha':[0.1,1,10,105,110,115],'fit_prior':[True,False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MnbTuned=GridSearchCV(mnb,TunedParamMnb,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.1, 1, 10, 105, 110, 115], 'fit_prior': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MnbTuned.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 110, 'fit_prior': True}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MnbTuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7861952861952862"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MnbTuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which hyper-parameters are important?\n",
    "\n",
    "For MutinomialNB, alpha and fit_prior are important hyper-parameters.\n",
    "\n",
    "alpha : Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "fit_prior :Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "\n",
    "* What hyper-parameter values work best? \n",
    "\n",
    "Through GridSearchCV, the best values of alpha is 110, fit_prior True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Which supervised learner works best on the test data? \n",
    "\n",
    "From above, I used some algoritms of supervised learning, decision tree, random forest, K-Nearest Neighbors and Naive Bayes.\n",
    "\n",
    "Through comparing the accuracy scores of each algorithm, random forest works best on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks can be used for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "y_train=to_categorical(y_train, n_classes)\n",
    "y_test=to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A shallow neutral network with 'sigmoid' activation and 'mean_squared_error' loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_net_A(n=100,i=10,o=3):\n",
    "    # create simple one dense layer net\n",
    "    # default 100 neurons, input 10, output 3\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='sigmoid', input_shape=(i,)))\n",
    "    net.add(Dense(o, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nna=shallow_net_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,403\n",
      "Trainable params: 1,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nna.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6534 samples, validate on 2179 samples\n",
      "Epoch 1/99\n",
      "6534/6534 [==============================] - 0s 53us/step - loss: 0.2216 - acc: 0.4480 - val_loss: 0.1899 - val_acc: 0.5571\n",
      "Epoch 2/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1883 - acc: 0.5450 - val_loss: 0.1850 - val_acc: 0.5571\n",
      "Epoch 3/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1851 - acc: 0.5450 - val_loss: 0.1836 - val_acc: 0.5571\n",
      "Epoch 4/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1835 - acc: 0.5450 - val_loss: 0.1818 - val_acc: 0.5571\n",
      "Epoch 5/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1821 - acc: 0.5450 - val_loss: 0.1807 - val_acc: 0.5571\n",
      "Epoch 6/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1809 - acc: 0.5450 - val_loss: 0.1793 - val_acc: 0.5571\n",
      "Epoch 7/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1797 - acc: 0.5450 - val_loss: 0.1783 - val_acc: 0.5571\n",
      "Epoch 8/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1786 - acc: 0.5450 - val_loss: 0.1773 - val_acc: 0.5571\n",
      "Epoch 9/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1775 - acc: 0.5450 - val_loss: 0.1759 - val_acc: 0.5571\n",
      "Epoch 10/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1764 - acc: 0.5450 - val_loss: 0.1750 - val_acc: 0.5571\n",
      "Epoch 11/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1754 - acc: 0.5450 - val_loss: 0.1741 - val_acc: 0.5571\n",
      "Epoch 12/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1744 - acc: 0.5450 - val_loss: 0.1732 - val_acc: 0.5571\n",
      "Epoch 13/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1734 - acc: 0.5450 - val_loss: 0.1729 - val_acc: 0.5571\n",
      "Epoch 14/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1725 - acc: 0.5450 - val_loss: 0.1717 - val_acc: 0.5571\n",
      "Epoch 15/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1715 - acc: 0.5526 - val_loss: 0.1703 - val_acc: 0.5571\n",
      "Epoch 16/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1705 - acc: 0.5450 - val_loss: 0.1691 - val_acc: 0.5571\n",
      "Epoch 17/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1696 - acc: 0.5523 - val_loss: 0.1685 - val_acc: 0.7292\n",
      "Epoch 18/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1687 - acc: 0.6327 - val_loss: 0.1674 - val_acc: 0.5571\n",
      "Epoch 19/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1679 - acc: 0.5912 - val_loss: 0.1664 - val_acc: 0.5571\n",
      "Epoch 20/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1670 - acc: 0.6350 - val_loss: 0.1659 - val_acc: 0.7292\n",
      "Epoch 21/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1661 - acc: 0.7179 - val_loss: 0.1648 - val_acc: 0.7292\n",
      "Epoch 22/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1654 - acc: 0.7227 - val_loss: 0.1640 - val_acc: 0.7292\n",
      "Epoch 23/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1646 - acc: 0.7227 - val_loss: 0.1637 - val_acc: 0.5571\n",
      "Epoch 24/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1640 - acc: 0.7146 - val_loss: 0.1626 - val_acc: 0.7292\n",
      "Epoch 25/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1631 - acc: 0.7227 - val_loss: 0.1618 - val_acc: 0.7292\n",
      "Epoch 26/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1623 - acc: 0.7227 - val_loss: 0.1611 - val_acc: 0.7292\n",
      "Epoch 27/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1616 - acc: 0.7227 - val_loss: 0.1611 - val_acc: 0.7292\n",
      "Epoch 28/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1610 - acc: 0.7227 - val_loss: 0.1598 - val_acc: 0.7292\n",
      "Epoch 29/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1603 - acc: 0.7227 - val_loss: 0.1593 - val_acc: 0.7292\n",
      "Epoch 30/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1597 - acc: 0.7227 - val_loss: 0.1592 - val_acc: 0.7292\n",
      "Epoch 31/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1591 - acc: 0.7227 - val_loss: 0.1596 - val_acc: 0.7292\n",
      "Epoch 32/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1585 - acc: 0.7227 - val_loss: 0.1588 - val_acc: 0.7292\n",
      "Epoch 33/99\n",
      "6534/6534 [==============================] - 0s 19us/step - loss: 0.1579 - acc: 0.7227 - val_loss: 0.1567 - val_acc: 0.7292\n",
      "Epoch 34/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1573 - acc: 0.7227 - val_loss: 0.1563 - val_acc: 0.7292\n",
      "Epoch 35/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1567 - acc: 0.7227 - val_loss: 0.1569 - val_acc: 0.7292\n",
      "Epoch 36/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1562 - acc: 0.7227 - val_loss: 0.1551 - val_acc: 0.7292\n",
      "Epoch 37/99\n",
      "6534/6534 [==============================] - 0s 23us/step - loss: 0.1558 - acc: 0.7227 - val_loss: 0.1546 - val_acc: 0.7292\n",
      "Epoch 38/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1552 - acc: 0.7227 - val_loss: 0.1544 - val_acc: 0.7292\n",
      "Epoch 39/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1547 - acc: 0.7227 - val_loss: 0.1536 - val_acc: 0.7292\n",
      "Epoch 40/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1543 - acc: 0.7227 - val_loss: 0.1531 - val_acc: 0.7292\n",
      "Epoch 41/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1537 - acc: 0.7227 - val_loss: 0.1533 - val_acc: 0.7292\n",
      "Epoch 42/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.1533 - acc: 0.7227 - val_loss: 0.1523 - val_acc: 0.7292\n",
      "Epoch 43/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.1529 - acc: 0.7227 - val_loss: 0.1518 - val_acc: 0.7292\n",
      "Epoch 44/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1525 - acc: 0.7227 - val_loss: 0.1513 - val_acc: 0.7292\n",
      "Epoch 45/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1520 - acc: 0.7227 - val_loss: 0.1513 - val_acc: 0.7292\n",
      "Epoch 46/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1517 - acc: 0.7227 - val_loss: 0.1511 - val_acc: 0.7292\n",
      "Epoch 47/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1513 - acc: 0.7227 - val_loss: 0.1501 - val_acc: 0.7292\n",
      "Epoch 48/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1509 - acc: 0.7227 - val_loss: 0.1501 - val_acc: 0.7292\n",
      "Epoch 49/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1505 - acc: 0.7227 - val_loss: 0.1494 - val_acc: 0.7292\n",
      "Epoch 50/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1502 - acc: 0.7227 - val_loss: 0.1493 - val_acc: 0.7292\n",
      "Epoch 51/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1498 - acc: 0.7227 - val_loss: 0.1487 - val_acc: 0.7292\n",
      "Epoch 52/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1495 - acc: 0.7227 - val_loss: 0.1490 - val_acc: 0.7292\n",
      "Epoch 53/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1492 - acc: 0.7227 - val_loss: 0.1482 - val_acc: 0.7292\n",
      "Epoch 54/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1488 - acc: 0.7227 - val_loss: 0.1480 - val_acc: 0.7292\n",
      "Epoch 55/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1485 - acc: 0.7227 - val_loss: 0.1475 - val_acc: 0.7292\n",
      "Epoch 56/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1482 - acc: 0.7227 - val_loss: 0.1471 - val_acc: 0.7292\n",
      "Epoch 57/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1479 - acc: 0.7227 - val_loss: 0.1469 - val_acc: 0.7292\n",
      "Epoch 58/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1476 - acc: 0.7227 - val_loss: 0.1472 - val_acc: 0.7292\n",
      "Epoch 59/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1474 - acc: 0.7227 - val_loss: 0.1465 - val_acc: 0.7292\n",
      "Epoch 60/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1472 - acc: 0.7227 - val_loss: 0.1462 - val_acc: 0.7292\n",
      "Epoch 61/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1469 - acc: 0.7227 - val_loss: 0.1459 - val_acc: 0.7292\n",
      "Epoch 62/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1467 - acc: 0.7227 - val_loss: 0.1461 - val_acc: 0.7292\n",
      "Epoch 63/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1464 - acc: 0.7227 - val_loss: 0.1457 - val_acc: 0.7292\n",
      "Epoch 64/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1461 - acc: 0.7227 - val_loss: 0.1458 - val_acc: 0.7292\n",
      "Epoch 65/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1460 - acc: 0.7227 - val_loss: 0.1456 - val_acc: 0.7292\n",
      "Epoch 66/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1457 - acc: 0.7227 - val_loss: 0.1446 - val_acc: 0.7292\n",
      "Epoch 67/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1454 - acc: 0.7227 - val_loss: 0.1444 - val_acc: 0.7292\n",
      "Epoch 68/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1452 - acc: 0.7227 - val_loss: 0.1443 - val_acc: 0.7292\n",
      "Epoch 69/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1450 - acc: 0.7227 - val_loss: 0.1442 - val_acc: 0.7292\n",
      "Epoch 70/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1448 - acc: 0.7227 - val_loss: 0.1441 - val_acc: 0.7292\n",
      "Epoch 71/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1446 - acc: 0.7227 - val_loss: 0.1436 - val_acc: 0.7292\n",
      "Epoch 72/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1445 - acc: 0.7227 - val_loss: 0.1435 - val_acc: 0.7292\n",
      "Epoch 73/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1442 - acc: 0.7227 - val_loss: 0.1432 - val_acc: 0.7292\n",
      "Epoch 74/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1440 - acc: 0.7227 - val_loss: 0.1432 - val_acc: 0.7292\n",
      "Epoch 75/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1438 - acc: 0.7227 - val_loss: 0.1428 - val_acc: 0.7292\n",
      "Epoch 76/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1437 - acc: 0.7227 - val_loss: 0.1427 - val_acc: 0.7292\n",
      "Epoch 77/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1435 - acc: 0.7227 - val_loss: 0.1426 - val_acc: 0.7292\n",
      "Epoch 78/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1434 - acc: 0.7227 - val_loss: 0.1425 - val_acc: 0.7292\n",
      "Epoch 79/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1432 - acc: 0.7227 - val_loss: 0.1422 - val_acc: 0.7292\n",
      "Epoch 80/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1431 - acc: 0.7227 - val_loss: 0.1422 - val_acc: 0.7292\n",
      "Epoch 81/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1429 - acc: 0.7227 - val_loss: 0.1419 - val_acc: 0.7292\n",
      "Epoch 82/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1427 - acc: 0.7227 - val_loss: 0.1423 - val_acc: 0.7292\n",
      "Epoch 83/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1426 - acc: 0.7227 - val_loss: 0.1417 - val_acc: 0.7292\n",
      "Epoch 84/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1424 - acc: 0.7227 - val_loss: 0.1417 - val_acc: 0.7292\n",
      "Epoch 85/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1423 - acc: 0.7227 - val_loss: 0.1413 - val_acc: 0.7292\n",
      "Epoch 86/99\n",
      "6534/6534 [==============================] - 0s 8us/step - loss: 0.1422 - acc: 0.7227 - val_loss: 0.1412 - val_acc: 0.7292\n",
      "Epoch 87/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1421 - acc: 0.7227 - val_loss: 0.1412 - val_acc: 0.7292\n",
      "Epoch 88/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1419 - acc: 0.7227 - val_loss: 0.1410 - val_acc: 0.7292\n",
      "Epoch 89/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.1418 - acc: 0.7227 - val_loss: 0.1408 - val_acc: 0.7292\n",
      "Epoch 90/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1417 - acc: 0.7227 - val_loss: 0.1409 - val_acc: 0.7292\n",
      "Epoch 91/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1415 - acc: 0.7227 - val_loss: 0.1407 - val_acc: 0.7292\n",
      "Epoch 92/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1415 - acc: 0.7227 - val_loss: 0.1405 - val_acc: 0.7292\n",
      "Epoch 93/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1414 - acc: 0.7227 - val_loss: 0.1408 - val_acc: 0.7292\n",
      "Epoch 94/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1413 - acc: 0.7227 - val_loss: 0.1410 - val_acc: 0.7292\n",
      "Epoch 95/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1412 - acc: 0.7227 - val_loss: 0.1402 - val_acc: 0.7292\n",
      "Epoch 96/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1410 - acc: 0.7227 - val_loss: 0.1401 - val_acc: 0.7292\n",
      "Epoch 97/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1409 - acc: 0.7227 - val_loss: 0.1400 - val_acc: 0.7292\n",
      "Epoch 98/99\n",
      "6534/6534 [==============================] - 0s 9us/step - loss: 0.1408 - acc: 0.7227 - val_loss: 0.1399 - val_acc: 0.7292\n",
      "Epoch 99/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.1408 - acc: 0.7227 - val_loss: 0.1399 - val_acc: 0.7292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c389f10f0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nna.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2179/2179 [==============================] - 0s 19us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1398930752725982, 0.729233593391464]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 73% accuracy after 99 epochs\n",
    "nna.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A shallow neutral network with 'relu' activation and 'mean_squared_error' loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_net_B(n=100,i=10,o=3):\n",
    "    # create simple one dense layer net\n",
    "    # default 100 neurons, input 10, output 3\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='relu', input_shape=(i,)))\n",
    "    net.add(Dense(o, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnb=shallow_net_B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,403\n",
      "Trainable params: 1,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6534 samples, validate on 2179 samples\n",
      "Epoch 1/99\n",
      "6534/6534 [==============================] - 0s 41us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 2/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 3/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 4/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 5/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 6/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 7/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 8/99\n",
      "6534/6534 [==============================] - 0s 10us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 9/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 10/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 11/99\n",
      "6534/6534 [==============================] - 0s 21us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 12/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 13/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 14/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 15/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 16/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 17/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 18/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 19/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 20/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 21/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 22/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 23/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 24/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 25/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 26/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 27/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 28/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 29/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 30/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 31/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 32/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 33/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 34/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 35/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 36/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 37/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 38/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 39/99\n",
      "6534/6534 [==============================] - 0s 19us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 40/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 41/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 42/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 43/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 44/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 45/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 46/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 47/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 48/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 49/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 50/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 51/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 52/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 53/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 54/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 55/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 56/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 57/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 58/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 59/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 60/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 62/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 63/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 64/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 65/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 66/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 67/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 68/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 69/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 70/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 71/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 72/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 73/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 74/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 75/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 76/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 77/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 78/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 79/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 80/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 81/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 82/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 83/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 84/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 85/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 86/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 87/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 88/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 89/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 90/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 91/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 92/99\n",
      "6534/6534 [==============================] - 0s 23us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 93/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 94/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 95/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 96/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 97/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 98/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n",
      "Epoch 99/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.3033 - acc: 0.5450 - val_loss: 0.2952 - val_acc: 0.5571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c38c2c208>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnb.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2179/2179 [==============================] - 0s 16us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29524247727163233, 0.5571363010828841]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 56% accuracy after 99 epochs\n",
    "nnb.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A shallow neutral network with 'tanh' activation and 'mean_squared_error' loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_net_C(n=100,i=10,o=3):\n",
    "    # create simple one dense layer net\n",
    "    # default 100 neurons, input 10, output 3\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='tanh', input_shape=(i,)))\n",
    "    net.add(Dense(o, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnc=shallow_net_C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,403\n",
      "Trainable params: 1,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6534 samples, validate on 2179 samples\n",
      "Epoch 1/99\n",
      "6534/6534 [==============================] - 0s 50us/step - loss: 0.2277 - acc: 0.5450 - val_loss: 0.2131 - val_acc: 0.5571\n",
      "Epoch 2/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.2105 - acc: 0.5450 - val_loss: 0.2036 - val_acc: 0.5571\n",
      "Epoch 3/99\n",
      "6534/6534 [==============================] - 0s 20us/step - loss: 0.2002 - acc: 0.5450 - val_loss: 0.1952 - val_acc: 0.5571\n",
      "Epoch 4/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1899 - acc: 0.5447 - val_loss: 0.1835 - val_acc: 0.5571\n",
      "Epoch 5/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1799 - acc: 0.5473 - val_loss: 0.1747 - val_acc: 0.5590\n",
      "Epoch 6/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1703 - acc: 0.5478 - val_loss: 0.1651 - val_acc: 0.5590\n",
      "Epoch 7/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1617 - acc: 0.5885 - val_loss: 0.1586 - val_acc: 0.5590\n",
      "Epoch 8/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1542 - acc: 0.7533 - val_loss: 0.1496 - val_acc: 0.8256\n",
      "Epoch 9/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1469 - acc: 0.8252 - val_loss: 0.1433 - val_acc: 0.8256\n",
      "Epoch 10/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.1410 - acc: 0.8252 - val_loss: 0.1386 - val_acc: 0.8256\n",
      "Epoch 11/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.1358 - acc: 0.8252 - val_loss: 0.1354 - val_acc: 0.8256\n",
      "Epoch 12/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1313 - acc: 0.8252 - val_loss: 0.1291 - val_acc: 0.8256\n",
      "Epoch 13/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1274 - acc: 0.8252 - val_loss: 0.1256 - val_acc: 0.8256\n",
      "Epoch 14/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1241 - acc: 0.8252 - val_loss: 0.1225 - val_acc: 0.8256\n",
      "Epoch 15/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.1213 - acc: 0.8252 - val_loss: 0.1207 - val_acc: 0.8256\n",
      "Epoch 16/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1189 - acc: 0.8252 - val_loss: 0.1177 - val_acc: 0.8256\n",
      "Epoch 17/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1167 - acc: 0.8252 - val_loss: 0.1160 - val_acc: 0.8256\n",
      "Epoch 18/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1149 - acc: 0.8252 - val_loss: 0.1140 - val_acc: 0.8256\n",
      "Epoch 19/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1133 - acc: 0.8252 - val_loss: 0.1135 - val_acc: 0.8256\n",
      "Epoch 20/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1119 - acc: 0.8252 - val_loss: 0.1113 - val_acc: 0.8256\n",
      "Epoch 21/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1107 - acc: 0.8252 - val_loss: 0.1104 - val_acc: 0.8256\n",
      "Epoch 22/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1096 - acc: 0.8252 - val_loss: 0.1092 - val_acc: 0.8256\n",
      "Epoch 23/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1086 - acc: 0.8252 - val_loss: 0.1087 - val_acc: 0.8256\n",
      "Epoch 24/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1077 - acc: 0.8252 - val_loss: 0.1075 - val_acc: 0.8256\n",
      "Epoch 25/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1070 - acc: 0.8252 - val_loss: 0.1069 - val_acc: 0.8256\n",
      "Epoch 26/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1063 - acc: 0.8252 - val_loss: 0.1061 - val_acc: 0.8256\n",
      "Epoch 27/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1057 - acc: 0.8252 - val_loss: 0.1056 - val_acc: 0.8256\n",
      "Epoch 28/99\n",
      "6534/6534 [==============================] - 0s 19us/step - loss: 0.1051 - acc: 0.8252 - val_loss: 0.1055 - val_acc: 0.8256\n",
      "Epoch 29/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1046 - acc: 0.8252 - val_loss: 0.1047 - val_acc: 0.8256\n",
      "Epoch 30/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1042 - acc: 0.8252 - val_loss: 0.1041 - val_acc: 0.8256\n",
      "Epoch 31/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1037 - acc: 0.8252 - val_loss: 0.1038 - val_acc: 0.8256\n",
      "Epoch 32/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1033 - acc: 0.8252 - val_loss: 0.1035 - val_acc: 0.8256\n",
      "Epoch 33/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1030 - acc: 0.8252 - val_loss: 0.1031 - val_acc: 0.8256\n",
      "Epoch 34/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.1027 - acc: 0.8252 - val_loss: 0.1030 - val_acc: 0.8256\n",
      "Epoch 35/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1024 - acc: 0.8252 - val_loss: 0.1025 - val_acc: 0.8256\n",
      "Epoch 36/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1021 - acc: 0.8252 - val_loss: 0.1023 - val_acc: 0.8256\n",
      "Epoch 37/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1019 - acc: 0.8252 - val_loss: 0.1020 - val_acc: 0.8256\n",
      "Epoch 38/99\n",
      "6534/6534 [==============================] - 0s 20us/step - loss: 0.1016 - acc: 0.8252 - val_loss: 0.1018 - val_acc: 0.8256\n",
      "Epoch 39/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.1014 - acc: 0.8252 - val_loss: 0.1017 - val_acc: 0.8256\n",
      "Epoch 40/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1013 - acc: 0.8252 - val_loss: 0.1015 - val_acc: 0.8256\n",
      "Epoch 41/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1010 - acc: 0.8252 - val_loss: 0.1013 - val_acc: 0.8256\n",
      "Epoch 42/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1009 - acc: 0.8252 - val_loss: 0.1011 - val_acc: 0.8256\n",
      "Epoch 43/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.1008 - acc: 0.8252 - val_loss: 0.1009 - val_acc: 0.8256\n",
      "Epoch 44/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1006 - acc: 0.8252 - val_loss: 0.1008 - val_acc: 0.8256\n",
      "Epoch 45/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1004 - acc: 0.8252 - val_loss: 0.1007 - val_acc: 0.8256\n",
      "Epoch 46/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.1003 - acc: 0.8252 - val_loss: 0.1005 - val_acc: 0.8256\n",
      "Epoch 47/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1002 - acc: 0.8252 - val_loss: 0.1010 - val_acc: 0.8256\n",
      "Epoch 48/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.1001 - acc: 0.8252 - val_loss: 0.1003 - val_acc: 0.8256\n",
      "Epoch 49/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0999 - acc: 0.8252 - val_loss: 0.1002 - val_acc: 0.8256\n",
      "Epoch 50/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0998 - acc: 0.8252 - val_loss: 0.1003 - val_acc: 0.8256\n",
      "Epoch 51/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0997 - acc: 0.8252 - val_loss: 0.1000 - val_acc: 0.8256\n",
      "Epoch 52/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0996 - acc: 0.8252 - val_loss: 0.1004 - val_acc: 0.8256\n",
      "Epoch 53/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0996 - acc: 0.8252 - val_loss: 0.0998 - val_acc: 0.8256\n",
      "Epoch 54/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0995 - acc: 0.8252 - val_loss: 0.0999 - val_acc: 0.8256\n",
      "Epoch 55/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0994 - acc: 0.8252 - val_loss: 0.1001 - val_acc: 0.8256\n",
      "Epoch 56/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.0993 - acc: 0.8252 - val_loss: 0.0996 - val_acc: 0.8256\n",
      "Epoch 57/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0992 - acc: 0.8252 - val_loss: 0.0996 - val_acc: 0.8256\n",
      "Epoch 58/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0992 - acc: 0.8252 - val_loss: 0.0995 - val_acc: 0.8256\n",
      "Epoch 59/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.0991 - acc: 0.8252 - val_loss: 0.0994 - val_acc: 0.8256\n",
      "Epoch 60/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0990 - acc: 0.8252 - val_loss: 0.0994 - val_acc: 0.8256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0990 - acc: 0.8252 - val_loss: 0.0993 - val_acc: 0.8256\n",
      "Epoch 62/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.0989 - acc: 0.8252 - val_loss: 0.0997 - val_acc: 0.8256\n",
      "Epoch 63/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0989 - acc: 0.8252 - val_loss: 0.0992 - val_acc: 0.8256\n",
      "Epoch 64/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0988 - acc: 0.8252 - val_loss: 0.0991 - val_acc: 0.8256\n",
      "Epoch 65/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0988 - acc: 0.8252 - val_loss: 0.0993 - val_acc: 0.8256\n",
      "Epoch 66/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0987 - acc: 0.8252 - val_loss: 0.0990 - val_acc: 0.8256\n",
      "Epoch 67/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0987 - acc: 0.8252 - val_loss: 0.0990 - val_acc: 0.8256\n",
      "Epoch 68/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0986 - acc: 0.8252 - val_loss: 0.0989 - val_acc: 0.8256\n",
      "Epoch 69/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0985 - acc: 0.8252 - val_loss: 0.0990 - val_acc: 0.8256\n",
      "Epoch 70/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.0985 - acc: 0.8252 - val_loss: 0.0989 - val_acc: 0.8256\n",
      "Epoch 71/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.0985 - acc: 0.8252 - val_loss: 0.0988 - val_acc: 0.8256\n",
      "Epoch 72/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.0984 - acc: 0.8252 - val_loss: 0.0990 - val_acc: 0.8256\n",
      "Epoch 73/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0984 - acc: 0.8252 - val_loss: 0.0987 - val_acc: 0.8256\n",
      "Epoch 74/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0983 - acc: 0.8252 - val_loss: 0.0988 - val_acc: 0.8256\n",
      "Epoch 75/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.0983 - acc: 0.8252 - val_loss: 0.0987 - val_acc: 0.8256\n",
      "Epoch 76/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0983 - acc: 0.8252 - val_loss: 0.0986 - val_acc: 0.8256\n",
      "Epoch 77/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.0982 - acc: 0.8252 - val_loss: 0.0986 - val_acc: 0.8256\n",
      "Epoch 78/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.0982 - acc: 0.8252 - val_loss: 0.0985 - val_acc: 0.8256\n",
      "Epoch 79/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0982 - acc: 0.8252 - val_loss: 0.0985 - val_acc: 0.8256\n",
      "Epoch 80/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.0981 - acc: 0.8252 - val_loss: 0.0987 - val_acc: 0.8256\n",
      "Epoch 81/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.0981 - acc: 0.8252 - val_loss: 0.0985 - val_acc: 0.8256\n",
      "Epoch 82/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.0981 - acc: 0.8252 - val_loss: 0.0984 - val_acc: 0.8256\n",
      "Epoch 83/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.0981 - acc: 0.8252 - val_loss: 0.0984 - val_acc: 0.8256\n",
      "Epoch 84/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.0980 - acc: 0.8252 - val_loss: 0.0984 - val_acc: 0.8256\n",
      "Epoch 85/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0980 - acc: 0.8252 - val_loss: 0.0985 - val_acc: 0.8256\n",
      "Epoch 86/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0979 - acc: 0.8252 - val_loss: 0.0983 - val_acc: 0.8256\n",
      "Epoch 87/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.0979 - acc: 0.8252 - val_loss: 0.0990 - val_acc: 0.8256\n",
      "Epoch 88/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0980 - acc: 0.8252 - val_loss: 0.0985 - val_acc: 0.8256\n",
      "Epoch 89/99\n",
      "6534/6534 [==============================] - 0s 22us/step - loss: 0.0979 - acc: 0.8252 - val_loss: 0.0984 - val_acc: 0.8256\n",
      "Epoch 90/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.0979 - acc: 0.8252 - val_loss: 0.0982 - val_acc: 0.8256\n",
      "Epoch 91/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.0978 - acc: 0.8252 - val_loss: 0.0982 - val_acc: 0.8256\n",
      "Epoch 92/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0978 - acc: 0.8252 - val_loss: 0.0982 - val_acc: 0.8256\n",
      "Epoch 93/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0978 - acc: 0.8252 - val_loss: 0.0981 - val_acc: 0.8256\n",
      "Epoch 94/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.0977 - acc: 0.8252 - val_loss: 0.0982 - val_acc: 0.8256\n",
      "Epoch 95/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0977 - acc: 0.8252 - val_loss: 0.0982 - val_acc: 0.8256\n",
      "Epoch 96/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0977 - acc: 0.8252 - val_loss: 0.0981 - val_acc: 0.8256\n",
      "Epoch 97/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.0977 - acc: 0.8252 - val_loss: 0.0981 - val_acc: 0.8256\n",
      "Epoch 98/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.0977 - acc: 0.8252 - val_loss: 0.0981 - val_acc: 0.8256\n",
      "Epoch 99/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.0976 - acc: 0.8252 - val_loss: 0.0980 - val_acc: 0.8256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c38e92be0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnc.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2179/2179 [==============================] - 0s 26us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09803839303901869, 0.825608077099587]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 83% accuracy after 99 epochs\n",
    "nnc.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A shallow neutral network with 'sigmoid' activation and 'categorical_crossentropy' loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_net_D(n=100,i=10,o=3):\n",
    "    # create simple one dense layer net\n",
    "    # default 100 neurons, input 10, output 3\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='sigmoid', input_shape=(i,)))\n",
    "    net.add(Dense(o, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnd=shallow_net_D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,403\n",
      "Trainable params: 1,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6534 samples, validate on 2179 samples\n",
      "Epoch 1/99\n",
      "6534/6534 [==============================] - 0s 66us/step - loss: 0.8607 - acc: 0.5450 - val_loss: 0.8418 - val_acc: 0.5571\n",
      "Epoch 2/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8484 - val_acc: 0.5571\n",
      "Epoch 3/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8391 - val_acc: 0.5571\n",
      "Epoch 4/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8656 - val_acc: 0.3947\n",
      "Epoch 5/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8418 - acc: 0.5401 - val_loss: 0.8490 - val_acc: 0.5571\n",
      "Epoch 6/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8421 - acc: 0.5450 - val_loss: 0.8409 - val_acc: 0.5571\n",
      "Epoch 7/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8417 - acc: 0.5450 - val_loss: 0.8475 - val_acc: 0.5571\n",
      "Epoch 8/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8422 - acc: 0.5450 - val_loss: 0.8391 - val_acc: 0.5571\n",
      "Epoch 9/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8410 - acc: 0.5450 - val_loss: 0.8498 - val_acc: 0.5571\n",
      "Epoch 10/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8430 - val_acc: 0.5571\n",
      "Epoch 11/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8409 - acc: 0.5450 - val_loss: 0.8454 - val_acc: 0.5571\n",
      "Epoch 12/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8417 - acc: 0.5450 - val_loss: 0.8392 - val_acc: 0.5571\n",
      "Epoch 13/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8412 - acc: 0.5450 - val_loss: 0.8391 - val_acc: 0.5571\n",
      "Epoch 14/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8416 - val_acc: 0.5571\n",
      "Epoch 15/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8417 - acc: 0.5450 - val_loss: 0.8399 - val_acc: 0.5571\n",
      "Epoch 16/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8410 - acc: 0.5450 - val_loss: 0.8418 - val_acc: 0.5571\n",
      "Epoch 17/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8418 - acc: 0.5450 - val_loss: 0.8448 - val_acc: 0.5571\n",
      "Epoch 18/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8417 - acc: 0.5450 - val_loss: 0.8407 - val_acc: 0.5571\n",
      "Epoch 19/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8391 - val_acc: 0.5571\n",
      "Epoch 20/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8405 - acc: 0.5450 - val_loss: 0.8421 - val_acc: 0.5571\n",
      "Epoch 21/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8390 - val_acc: 0.5571\n",
      "Epoch 22/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8449 - val_acc: 0.5571\n",
      "Epoch 23/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8417 - acc: 0.5450 - val_loss: 0.8394 - val_acc: 0.5571\n",
      "Epoch 24/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8392 - val_acc: 0.5571\n",
      "Epoch 25/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8412 - acc: 0.5450 - val_loss: 0.8421 - val_acc: 0.5571\n",
      "Epoch 26/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8410 - acc: 0.5450 - val_loss: 0.8395 - val_acc: 0.5571\n",
      "Epoch 27/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8412 - val_acc: 0.5571\n",
      "Epoch 28/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8392 - val_acc: 0.5571\n",
      "Epoch 29/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8392 - val_acc: 0.5571\n",
      "Epoch 30/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8404 - val_acc: 0.5571\n",
      "Epoch 31/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8402 - val_acc: 0.5571\n",
      "Epoch 32/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8412 - acc: 0.5450 - val_loss: 0.8443 - val_acc: 0.5571\n",
      "Epoch 33/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8419 - acc: 0.5450 - val_loss: 0.8404 - val_acc: 0.5571\n",
      "Epoch 34/99\n",
      "6534/6534 [==============================] - ETA: 0s - loss: 0.8367 - acc: 0.550 - 0s 15us/step - loss: 0.8412 - acc: 0.5450 - val_loss: 0.8469 - val_acc: 0.5571\n",
      "Epoch 35/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8419 - acc: 0.5450 - val_loss: 0.8505 - val_acc: 0.5571\n",
      "Epoch 36/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8590 - val_acc: 0.5571\n",
      "Epoch 37/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8420 - acc: 0.5450 - val_loss: 0.8392 - val_acc: 0.5571\n",
      "Epoch 38/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8411 - acc: 0.5450 - val_loss: 0.8548 - val_acc: 0.3947\n",
      "Epoch 39/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8422 - acc: 0.5432 - val_loss: 0.8404 - val_acc: 0.5571\n",
      "Epoch 40/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8579 - val_acc: 0.5571\n",
      "Epoch 41/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8428 - acc: 0.5450 - val_loss: 0.8902 - val_acc: 0.3947\n",
      "Epoch 42/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8432 - acc: 0.5369 - val_loss: 0.8455 - val_acc: 0.5571\n",
      "Epoch 43/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8422 - acc: 0.5450 - val_loss: 0.8478 - val_acc: 0.5571\n",
      "Epoch 44/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8417 - acc: 0.5450 - val_loss: 0.8463 - val_acc: 0.5571\n",
      "Epoch 45/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8535 - val_acc: 0.3947\n",
      "Epoch 46/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8416 - acc: 0.5436 - val_loss: 0.8434 - val_acc: 0.5571\n",
      "Epoch 47/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8396 - val_acc: 0.5571\n",
      "Epoch 48/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8412 - acc: 0.5450 - val_loss: 0.8393 - val_acc: 0.5571\n",
      "Epoch 49/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8391 - val_acc: 0.5571\n",
      "Epoch 50/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8418 - val_acc: 0.5571\n",
      "Epoch 51/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8411 - acc: 0.5450 - val_loss: 0.8406 - val_acc: 0.5571\n",
      "Epoch 52/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8421 - val_acc: 0.5571\n",
      "Epoch 53/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8397 - val_acc: 0.5571\n",
      "Epoch 54/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8411 - acc: 0.5450 - val_loss: 0.8403 - val_acc: 0.5571\n",
      "Epoch 55/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8407 - val_acc: 0.5571\n",
      "Epoch 56/99\n",
      "6534/6534 [==============================] - 0s 19us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8420 - val_acc: 0.5571\n",
      "Epoch 57/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8412 - val_acc: 0.5571\n",
      "Epoch 58/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8412 - acc: 0.5450 - val_loss: 0.8544 - val_acc: 0.5571\n",
      "Epoch 59/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8423 - acc: 0.5450 - val_loss: 0.8390 - val_acc: 0.5571\n",
      "Epoch 60/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8476 - val_acc: 0.5571\n",
      "Epoch 61/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8424 - acc: 0.5450 - val_loss: 0.8452 - val_acc: 0.5571\n",
      "Epoch 62/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8453 - val_acc: 0.5571\n",
      "Epoch 63/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8392 - val_acc: 0.5571\n",
      "Epoch 64/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8480 - val_acc: 0.5571\n",
      "Epoch 65/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8491 - val_acc: 0.5571\n",
      "Epoch 66/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8417 - acc: 0.5450 - val_loss: 0.8425 - val_acc: 0.5571\n",
      "Epoch 67/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8448 - val_acc: 0.5571\n",
      "Epoch 68/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8390 - val_acc: 0.5571\n",
      "Epoch 69/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8421 - val_acc: 0.5571\n",
      "Epoch 70/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8417 - acc: 0.5450 - val_loss: 0.8403 - val_acc: 0.5571\n",
      "Epoch 71/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8393 - val_acc: 0.5571\n",
      "Epoch 72/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8504 - val_acc: 0.5571\n",
      "Epoch 73/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8612 - val_acc: 0.3947\n",
      "Epoch 74/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8420 - acc: 0.5421 - val_loss: 0.8409 - val_acc: 0.5571\n",
      "Epoch 75/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8402 - val_acc: 0.5571\n",
      "Epoch 76/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8411 - acc: 0.5450 - val_loss: 0.8402 - val_acc: 0.5571\n",
      "Epoch 77/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8409 - val_acc: 0.5571\n",
      "Epoch 78/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8419 - acc: 0.5450 - val_loss: 0.8410 - val_acc: 0.5571\n",
      "Epoch 79/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8390 - val_acc: 0.5571\n",
      "Epoch 80/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8417 - val_acc: 0.5571\n",
      "Epoch 81/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8410 - acc: 0.5450 - val_loss: 0.8421 - val_acc: 0.5571\n",
      "Epoch 82/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8481 - val_acc: 0.5571\n",
      "Epoch 83/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8412 - acc: 0.5450 - val_loss: 0.8408 - val_acc: 0.5571\n",
      "Epoch 84/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8390 - val_acc: 0.5571\n",
      "Epoch 85/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8453 - val_acc: 0.5571\n",
      "Epoch 86/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8408 - acc: 0.5450 - val_loss: 0.8401 - val_acc: 0.5571\n",
      "Epoch 87/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8415 - val_acc: 0.5571\n",
      "Epoch 88/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8397 - val_acc: 0.5571\n",
      "Epoch 89/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8400 - val_acc: 0.5571\n",
      "Epoch 90/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8392 - val_acc: 0.5571\n",
      "Epoch 91/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8414 - acc: 0.5450 - val_loss: 0.8609 - val_acc: 0.3947\n",
      "Epoch 92/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8419 - acc: 0.5416 - val_loss: 0.8427 - val_acc: 0.5571\n",
      "Epoch 93/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8405 - acc: 0.5450 - val_loss: 0.8462 - val_acc: 0.5571\n",
      "Epoch 94/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8418 - acc: 0.5450 - val_loss: 0.8405 - val_acc: 0.5571\n",
      "Epoch 95/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8416 - acc: 0.5450 - val_loss: 0.8488 - val_acc: 0.5571\n",
      "Epoch 96/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8415 - acc: 0.5450 - val_loss: 0.8394 - val_acc: 0.5571\n",
      "Epoch 97/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8409 - acc: 0.5450 - val_loss: 0.8536 - val_acc: 0.3947\n",
      "Epoch 98/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8418 - acc: 0.5413 - val_loss: 0.8396 - val_acc: 0.5571\n",
      "Epoch 99/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.8413 - acc: 0.5450 - val_loss: 0.8391 - val_acc: 0.5571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c390a9860>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnd.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2179/2179 [==============================] - 0s 21us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.858998554871136, 0.5571363010828841]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 56% accuracy after 99 epochs\n",
    "nnd.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A shallow neutral network with 'relu' activation and 'categorical_crossentropy' loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_net_E(n=100,i=10,o=3):\n",
    "    # create simple one dense layer net\n",
    "    # default 100 neurons, input 10, output 3\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='relu', input_shape=(i,)))\n",
    "    net.add(Dense(o, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nne=shallow_net_E()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,403\n",
      "Trainable params: 1,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nne.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6534 samples, validate on 2179 samples\n",
      "Epoch 1/99\n",
      "6534/6534 [==============================] - 0s 68us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 2/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 3/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 4/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 5/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 6/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 7/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 8/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 9/99\n",
      "6534/6534 [==============================] - 0s 22us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 10/99\n",
      "6534/6534 [==============================] - 0s 22us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 11/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 12/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 13/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 14/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 15/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 16/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 17/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 18/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 19/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 20/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 21/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 22/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 23/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 24/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 25/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 26/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 27/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 28/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 29/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 30/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 31/99\n",
      "6534/6534 [==============================] - 0s 19us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 32/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 33/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 34/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 35/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 36/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 37/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 38/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 39/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 40/99\n",
      "6534/6534 [==============================] - 0s 19us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 41/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 42/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 43/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 44/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 45/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 46/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 47/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 48/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 49/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 50/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 51/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 52/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 53/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 54/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 55/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 56/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 57/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 58/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 59/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 60/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 62/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 63/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 64/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 65/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 66/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 67/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 68/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 69/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 70/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 71/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 72/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 73/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 74/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 75/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 76/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 77/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 78/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 79/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 80/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 81/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 82/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 83/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 84/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 85/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 86/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 87/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 88/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 89/99\n",
      "6534/6534 [==============================] - 0s 20us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 90/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 91/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 92/99\n",
      "6534/6534 [==============================] - 0s 19us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 93/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 94/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 95/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 96/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 97/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 98/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n",
      "Epoch 99/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 7.3338 - acc: 0.5450 - val_loss: 7.1381 - val_acc: 0.5571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3939a588>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nne.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2179/2179 [==============================] - 0s 17us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.138119484208603, 0.5571363010828841]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 56% accuracy after 99 epochs\n",
    "nne.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A shallow neutral network with 'tanh' activation and 'categorical_crossentropy' loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_net_F(n=100,i=10,o=3):\n",
    "    # create simple one dense layer net\n",
    "    # default 100 neurons, input 10, output 3\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='tanh', input_shape=(i,)))\n",
    "    net.add(Dense(o, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnf=shallow_net_F()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,403\n",
      "Trainable params: 1,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6534 samples, validate on 2179 samples\n",
      "Epoch 1/99\n",
      "6534/6534 [==============================] - 0s 64us/step - loss: 0.8617 - acc: 0.5725 - val_loss: 0.8407 - val_acc: 0.5571\n",
      "Epoch 2/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8425 - acc: 0.5450 - val_loss: 0.8464 - val_acc: 0.5571\n",
      "Epoch 3/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8417 - acc: 0.5450 - val_loss: 0.8706 - val_acc: 0.5571\n",
      "Epoch 4/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8419 - acc: 0.5450 - val_loss: 0.8550 - val_acc: 0.5571\n",
      "Epoch 5/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8421 - acc: 0.5450 - val_loss: 0.8873 - val_acc: 0.5571\n",
      "Epoch 6/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8434 - acc: 0.5450 - val_loss: 0.8420 - val_acc: 0.5571\n",
      "Epoch 7/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8418 - acc: 0.5450 - val_loss: 0.9208 - val_acc: 0.3947\n",
      "Epoch 8/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8439 - acc: 0.5355 - val_loss: 0.8449 - val_acc: 0.5571\n",
      "Epoch 9/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8420 - acc: 0.5450 - val_loss: 0.8444 - val_acc: 0.5571\n",
      "Epoch 10/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8418 - acc: 0.5450 - val_loss: 0.8422 - val_acc: 0.5571\n",
      "Epoch 11/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8424 - acc: 0.5450 - val_loss: 0.8435 - val_acc: 0.5571\n",
      "Epoch 12/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8419 - acc: 0.5450 - val_loss: 0.8654 - val_acc: 0.3947\n",
      "Epoch 13/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8421 - acc: 0.5409 - val_loss: 0.8413 - val_acc: 0.5571\n",
      "Epoch 14/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8412 - acc: 0.5453 - val_loss: 0.8419 - val_acc: 0.5571\n",
      "Epoch 15/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8409 - acc: 0.5455 - val_loss: 0.8658 - val_acc: 0.5571\n",
      "Epoch 16/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8428 - acc: 0.5456 - val_loss: 0.8446 - val_acc: 0.5571\n",
      "Epoch 17/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8421 - acc: 0.5453 - val_loss: 0.8618 - val_acc: 0.5571\n",
      "Epoch 18/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8419 - acc: 0.5451 - val_loss: 0.8557 - val_acc: 0.5571\n",
      "Epoch 19/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8430 - acc: 0.5448 - val_loss: 0.8682 - val_acc: 0.5571\n",
      "Epoch 20/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8421 - acc: 0.5450 - val_loss: 0.8652 - val_acc: 0.3947\n",
      "Epoch 21/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8423 - acc: 0.5413 - val_loss: 0.8874 - val_acc: 0.5571\n",
      "Epoch 22/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8423 - acc: 0.5468 - val_loss: 0.8405 - val_acc: 0.5571\n",
      "Epoch 23/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8401 - acc: 0.5471 - val_loss: 0.8416 - val_acc: 0.5571\n",
      "Epoch 24/99\n",
      "6534/6534 [==============================] - ETA: 0s - loss: 0.8426 - acc: 0.543 - 0s 15us/step - loss: 0.8417 - acc: 0.5478 - val_loss: 0.8437 - val_acc: 0.5571\n",
      "Epoch 25/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8422 - acc: 0.5467 - val_loss: 0.8468 - val_acc: 0.5571\n",
      "Epoch 26/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8410 - acc: 0.5373 - val_loss: 0.8687 - val_acc: 0.3947\n",
      "Epoch 27/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8424 - acc: 0.5429 - val_loss: 0.8517 - val_acc: 0.5576\n",
      "Epoch 28/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8424 - acc: 0.5456 - val_loss: 0.8620 - val_acc: 0.5571\n",
      "Epoch 29/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8419 - acc: 0.5488 - val_loss: 0.8606 - val_acc: 0.5571\n",
      "Epoch 30/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8413 - acc: 0.5464 - val_loss: 0.8418 - val_acc: 0.5576\n",
      "Epoch 31/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8413 - acc: 0.5481 - val_loss: 0.8480 - val_acc: 0.5576\n",
      "Epoch 32/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8412 - acc: 0.5471 - val_loss: 0.8391 - val_acc: 0.5571\n",
      "Epoch 33/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8406 - acc: 0.5474 - val_loss: 0.8796 - val_acc: 0.3947\n",
      "Epoch 34/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8418 - acc: 0.5459 - val_loss: 0.8424 - val_acc: 0.5571\n",
      "Epoch 35/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8410 - acc: 0.5479 - val_loss: 0.9146 - val_acc: 0.3947\n",
      "Epoch 36/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8433 - acc: 0.5401 - val_loss: 0.8440 - val_acc: 0.5576\n",
      "Epoch 37/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8408 - acc: 0.5493 - val_loss: 0.8719 - val_acc: 0.3947\n",
      "Epoch 38/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8414 - acc: 0.5470 - val_loss: 0.8410 - val_acc: 0.5576\n",
      "Epoch 39/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8407 - acc: 0.5490 - val_loss: 0.8395 - val_acc: 0.5571\n",
      "Epoch 40/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8414 - acc: 0.5491 - val_loss: 0.8581 - val_acc: 0.5571\n",
      "Epoch 41/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8411 - acc: 0.5479 - val_loss: 0.8406 - val_acc: 0.5576\n",
      "Epoch 42/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8418 - acc: 0.5488 - val_loss: 0.8456 - val_acc: 0.5576\n",
      "Epoch 43/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8408 - acc: 0.5487 - val_loss: 0.9088 - val_acc: 0.5571\n",
      "Epoch 44/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8428 - acc: 0.5491 - val_loss: 0.8437 - val_acc: 0.5576\n",
      "Epoch 45/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8416 - acc: 0.5490 - val_loss: 0.8629 - val_acc: 0.5571\n",
      "Epoch 46/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8413 - acc: 0.5488 - val_loss: 0.8775 - val_acc: 0.3947\n",
      "Epoch 47/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8418 - acc: 0.5451 - val_loss: 0.8517 - val_acc: 0.5576\n",
      "Epoch 48/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8419 - acc: 0.5456 - val_loss: 0.8435 - val_acc: 0.5576\n",
      "Epoch 49/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8405 - acc: 0.5497 - val_loss: 0.8684 - val_acc: 0.5571\n",
      "Epoch 50/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8418 - acc: 0.5490 - val_loss: 0.8404 - val_acc: 0.5576\n",
      "Epoch 51/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8407 - acc: 0.5497 - val_loss: 0.8629 - val_acc: 0.3947\n",
      "Epoch 52/99\n",
      "6534/6534 [==============================] - ETA: 0s - loss: 0.8340 - acc: 0.552 - 0s 16us/step - loss: 0.8414 - acc: 0.5459 - val_loss: 0.9485 - val_acc: 0.3947\n",
      "Epoch 53/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8442 - acc: 0.5415 - val_loss: 0.8396 - val_acc: 0.5576\n",
      "Epoch 54/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8410 - acc: 0.5497 - val_loss: 0.8425 - val_acc: 0.5576\n",
      "Epoch 55/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8406 - acc: 0.5444 - val_loss: 0.8407 - val_acc: 0.5576\n",
      "Epoch 56/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8413 - acc: 0.5496 - val_loss: 0.8557 - val_acc: 0.3947\n",
      "Epoch 57/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8405 - acc: 0.5456 - val_loss: 0.8453 - val_acc: 0.5576\n",
      "Epoch 58/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8406 - acc: 0.5497 - val_loss: 0.8537 - val_acc: 0.5576\n",
      "Epoch 59/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8418 - acc: 0.5497 - val_loss: 0.8422 - val_acc: 0.5576\n",
      "Epoch 60/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8410 - acc: 0.5419 - val_loss: 0.8529 - val_acc: 0.5576\n",
      "Epoch 61/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8422 - acc: 0.5496 - val_loss: 0.8478 - val_acc: 0.5571\n",
      "Epoch 62/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8407 - acc: 0.5494 - val_loss: 0.8596 - val_acc: 0.5571\n",
      "Epoch 63/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8415 - acc: 0.5496 - val_loss: 0.8391 - val_acc: 0.5576\n",
      "Epoch 64/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8416 - acc: 0.5497 - val_loss: 0.8612 - val_acc: 0.5571\n",
      "Epoch 65/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8404 - acc: 0.5497 - val_loss: 0.8534 - val_acc: 0.5576\n",
      "Epoch 66/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8410 - acc: 0.5497 - val_loss: 0.8451 - val_acc: 0.5576\n",
      "Epoch 67/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8417 - acc: 0.5497 - val_loss: 0.8418 - val_acc: 0.5576\n",
      "Epoch 68/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8411 - acc: 0.5453 - val_loss: 0.8722 - val_acc: 0.5571\n",
      "Epoch 69/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8423 - acc: 0.5494 - val_loss: 0.8406 - val_acc: 0.5576\n",
      "Epoch 70/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8405 - acc: 0.5497 - val_loss: 0.9253 - val_acc: 0.5571\n",
      "Epoch 71/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8437 - acc: 0.5488 - val_loss: 0.8390 - val_acc: 0.5576\n",
      "Epoch 72/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8402 - acc: 0.5465 - val_loss: 0.8539 - val_acc: 0.3947\n",
      "Epoch 73/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8413 - acc: 0.5474 - val_loss: 0.8399 - val_acc: 0.5576\n",
      "Epoch 74/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8405 - acc: 0.5497 - val_loss: 0.8443 - val_acc: 0.5576\n",
      "Epoch 75/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8408 - acc: 0.5497 - val_loss: 0.8706 - val_acc: 0.3947\n",
      "Epoch 76/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8410 - acc: 0.5462 - val_loss: 0.8438 - val_acc: 0.5576\n",
      "Epoch 77/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8405 - acc: 0.5497 - val_loss: 0.8415 - val_acc: 0.5576\n",
      "Epoch 78/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8408 - acc: 0.5497 - val_loss: 0.8594 - val_acc: 0.3947\n",
      "Epoch 79/99\n",
      "6534/6534 [==============================] - 0s 15us/step - loss: 0.8412 - acc: 0.5478 - val_loss: 0.8449 - val_acc: 0.5576\n",
      "Epoch 80/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8407 - acc: 0.5464 - val_loss: 0.8419 - val_acc: 0.5576\n",
      "Epoch 81/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8401 - acc: 0.5497 - val_loss: 0.8446 - val_acc: 0.5576\n",
      "Epoch 82/99\n",
      "6534/6534 [==============================] - 0s 12us/step - loss: 0.8409 - acc: 0.5497 - val_loss: 0.8445 - val_acc: 0.5576\n",
      "Epoch 83/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.8404 - acc: 0.5497 - val_loss: 0.8591 - val_acc: 0.5571\n",
      "Epoch 84/99\n",
      "6534/6534 [==============================] - 0s 11us/step - loss: 0.8411 - acc: 0.5496 - val_loss: 0.8639 - val_acc: 0.3947\n",
      "Epoch 85/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8398 - acc: 0.5468 - val_loss: 0.8819 - val_acc: 0.5571\n",
      "Epoch 86/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8429 - acc: 0.5496 - val_loss: 0.8631 - val_acc: 0.5571\n",
      "Epoch 87/99\n",
      "6534/6534 [==============================] - 0s 14us/step - loss: 0.8409 - acc: 0.5497 - val_loss: 0.8443 - val_acc: 0.5576\n",
      "Epoch 88/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8416 - acc: 0.5497 - val_loss: 0.8509 - val_acc: 0.5576\n",
      "Epoch 89/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8415 - acc: 0.5497 - val_loss: 0.8410 - val_acc: 0.5576\n",
      "Epoch 90/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8409 - acc: 0.5497 - val_loss: 0.8420 - val_acc: 0.5576\n",
      "Epoch 91/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8398 - acc: 0.5497 - val_loss: 0.8400 - val_acc: 0.5576\n",
      "Epoch 92/99\n",
      "6534/6534 [==============================] - 0s 18us/step - loss: 0.8402 - acc: 0.5497 - val_loss: 0.8421 - val_acc: 0.5576\n",
      "Epoch 93/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8403 - acc: 0.5497 - val_loss: 0.8413 - val_acc: 0.5576\n",
      "Epoch 94/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8397 - acc: 0.5497 - val_loss: 0.8619 - val_acc: 0.5571\n",
      "Epoch 95/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8419 - acc: 0.5496 - val_loss: 0.8542 - val_acc: 0.5576\n",
      "Epoch 96/99\n",
      "6534/6534 [==============================] - 0s 13us/step - loss: 0.8412 - acc: 0.5497 - val_loss: 0.8554 - val_acc: 0.5576\n",
      "Epoch 97/99\n",
      "6534/6534 [==============================] - 0s 19us/step - loss: 0.8408 - acc: 0.5497 - val_loss: 0.8459 - val_acc: 0.5576\n",
      "Epoch 98/99\n",
      "6534/6534 [==============================] - 0s 17us/step - loss: 0.8404 - acc: 0.5497 - val_loss: 0.8675 - val_acc: 0.3947\n",
      "Epoch 99/99\n",
      "6534/6534 [==============================] - 0s 16us/step - loss: 0.8407 - acc: 0.5471 - val_loss: 0.8635 - val_acc: 0.5571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c392a85c0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnf.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2179/2179 [==============================] - 0s 22us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8634638084135454, 0.5571363010828841]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 56% accuracy after 99 epochs\n",
    "nnf.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which hyper-parameters are important?\n",
    "\n",
    "For neutral network, the number of neurons, activation function and loss function matter.\n",
    "\n",
    "* What hyper-parameter values work best? \n",
    "\n",
    "Through all results we got above, it's apparent that when we chose 'tanh' activation function and 'mean_squared_error' loss function, the accuracy is highest, 83%.\n",
    "\n",
    "* How the the neural network compare to the supervised learners in part A?   \n",
    "\n",
    "Comparing to the supervised learners in part A, the neutral network works better than naive bayes, appropriate to K-Nearest Neighbors and worse than decision tree and random forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunedParamMlp={'hidden_layer_sizes':[(100,),(55,55),(20,20,20)],'activation':['logistic','relu','tanh','identity'],'alpha':[0.000001,0.00001,0.0001,0.001,0.01],'solver':['lbfgs','sgd','adam'],'batch_size':[100,200,300],'verbose':[True,False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpTuned=RandomizedSearchCV(mlp,tunedParamMlp,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.88108151\n",
      "Iteration 2, loss = 1.61648404\n",
      "Iteration 3, loss = 1.57029400\n",
      "Iteration 4, loss = 1.56001696\n",
      "Iteration 5, loss = 1.55738079\n",
      "Iteration 6, loss = 1.55567102\n",
      "Iteration 7, loss = 1.55453287\n",
      "Iteration 8, loss = 1.55454607\n",
      "Iteration 9, loss = 1.55609696\n",
      "Iteration 10, loss = 1.55532261\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.83293918\n",
      "Iteration 2, loss = 1.58672112\n",
      "Iteration 3, loss = 1.56041211\n",
      "Iteration 4, loss = 1.55590446\n",
      "Iteration 5, loss = 1.55487199\n",
      "Iteration 6, loss = 1.55506084\n",
      "Iteration 7, loss = 1.55462504\n",
      "Iteration 8, loss = 1.55474503\n",
      "Iteration 9, loss = 1.55460667\n",
      "Iteration 10, loss = 1.55474802\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91054767\n",
      "Iteration 2, loss = 1.61434833\n",
      "Iteration 3, loss = 1.56811052\n",
      "Iteration 4, loss = 1.55936212\n",
      "Iteration 5, loss = 1.55834594\n",
      "Iteration 6, loss = 1.55623549\n",
      "Iteration 7, loss = 1.55680773\n",
      "Iteration 8, loss = 1.55652393\n",
      "Iteration 9, loss = 1.55619391\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.16340246\n",
      "Iteration 2, loss = 1.67767903\n",
      "Iteration 3, loss = 1.58921378\n",
      "Iteration 4, loss = 1.56952830\n",
      "Iteration 5, loss = 1.56328068\n",
      "Iteration 6, loss = 1.55977244\n",
      "Iteration 7, loss = 1.55872533\n",
      "Iteration 8, loss = 1.55865169\n",
      "Iteration 9, loss = 1.55782351\n",
      "Iteration 10, loss = 1.55915959\n",
      "Iteration 11, loss = 1.55852530\n",
      "Iteration 12, loss = 1.55839754\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.87442936\n",
      "Iteration 2, loss = 1.60180618\n",
      "Iteration 3, loss = 1.54936193\n",
      "Iteration 4, loss = 1.53337754\n",
      "Iteration 5, loss = 1.52084118\n",
      "Iteration 6, loss = 1.50762280\n",
      "Iteration 7, loss = 1.49062112\n",
      "Iteration 8, loss = 1.46846818\n",
      "Iteration 9, loss = 1.44366982\n",
      "Iteration 10, loss = 1.41305213\n",
      "Iteration 11, loss = 1.37866537\n",
      "Iteration 12, loss = 1.34627323\n",
      "Iteration 13, loss = 1.30702463\n",
      "Iteration 14, loss = 1.27019704\n",
      "Iteration 15, loss = 1.23416045\n",
      "Iteration 16, loss = 1.19690565\n",
      "Iteration 17, loss = 1.16425155\n",
      "Iteration 18, loss = 1.13260344\n",
      "Iteration 19, loss = 1.10860117\n",
      "Iteration 20, loss = 1.08615221\n",
      "Iteration 21, loss = 1.06561501\n",
      "Iteration 22, loss = 1.05096536\n",
      "Iteration 23, loss = 1.03875696\n",
      "Iteration 24, loss = 1.02917666\n",
      "Iteration 25, loss = 1.01912597\n",
      "Iteration 26, loss = 1.01208216\n",
      "Iteration 27, loss = 1.00594871\n",
      "Iteration 28, loss = 1.00035715\n",
      "Iteration 29, loss = 0.99704036\n",
      "Iteration 30, loss = 0.99421814\n",
      "Iteration 31, loss = 0.99205546\n",
      "Iteration 32, loss = 0.98954373\n",
      "Iteration 33, loss = 0.98843886\n",
      "Iteration 34, loss = 0.98677624\n",
      "Iteration 35, loss = 0.98501448\n",
      "Iteration 36, loss = 0.98852538\n",
      "Iteration 37, loss = 0.98486901\n",
      "Iteration 38, loss = 0.98397609\n",
      "Iteration 39, loss = 0.98274875\n",
      "Iteration 40, loss = 0.98156620\n",
      "Iteration 41, loss = 0.98447622\n",
      "Iteration 42, loss = 0.98297857\n",
      "Iteration 43, loss = 0.98184263\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.63418837\n",
      "Iteration 2, loss = 1.55062568\n",
      "Iteration 3, loss = 1.53773708\n",
      "Iteration 4, loss = 1.52776981\n",
      "Iteration 5, loss = 1.51407339\n",
      "Iteration 6, loss = 1.49601999\n",
      "Iteration 7, loss = 1.45656582\n",
      "Iteration 8, loss = 1.40467600\n",
      "Iteration 9, loss = 1.34520817\n",
      "Iteration 10, loss = 1.34615731\n",
      "Iteration 11, loss = 1.32890548\n",
      "Iteration 12, loss = 1.28452455\n",
      "Iteration 13, loss = 1.24177506\n",
      "Iteration 14, loss = 1.20065135\n",
      "Iteration 15, loss = 1.16420276\n",
      "Iteration 16, loss = 1.13081556\n",
      "Iteration 17, loss = 1.10118467\n",
      "Iteration 18, loss = 1.07674780\n",
      "Iteration 19, loss = 1.05720276\n",
      "Iteration 20, loss = 1.03993710\n",
      "Iteration 21, loss = 1.02855659\n",
      "Iteration 22, loss = 1.01911514\n",
      "Iteration 23, loss = 1.01060719\n",
      "Iteration 24, loss = 1.00284492\n",
      "Iteration 25, loss = 0.99660439\n",
      "Iteration 26, loss = 0.99272062\n",
      "Iteration 27, loss = 0.99398829\n",
      "Iteration 28, loss = 0.98877448\n",
      "Iteration 29, loss = 0.98731861\n",
      "Iteration 30, loss = 0.98409854\n",
      "Iteration 31, loss = 0.98338114\n",
      "Iteration 32, loss = 0.98379095\n",
      "Iteration 33, loss = 0.98101738\n",
      "Iteration 34, loss = 0.98191542\n",
      "Iteration 35, loss = 0.98021618\n",
      "Iteration 36, loss = 0.97982058\n",
      "Iteration 37, loss = 0.98014833\n",
      "Iteration 38, loss = 0.97907833\n",
      "Iteration 39, loss = 0.97828220\n",
      "Iteration 40, loss = 0.97865448\n",
      "Iteration 41, loss = 0.98037244\n",
      "Iteration 42, loss = 0.97736622\n",
      "Iteration 43, loss = 0.97855206\n",
      "Iteration 44, loss = 0.97846990\n",
      "Iteration 45, loss = 0.97790708\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.69149081\n",
      "Iteration 2, loss = 1.56103755\n",
      "Iteration 3, loss = 1.55322982\n",
      "Iteration 4, loss = 1.55198591\n",
      "Iteration 5, loss = 1.55222706\n",
      "Iteration 6, loss = 1.55194743\n",
      "Iteration 7, loss = 1.55170027\n",
      "Iteration 8, loss = 1.55249735\n",
      "Iteration 9, loss = 1.55244340\n",
      "Iteration 10, loss = 1.55180897\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.73593609\n",
      "Iteration 2, loss = 1.55093004\n",
      "Iteration 3, loss = 1.51657800\n",
      "Iteration 4, loss = 1.49069328\n",
      "Iteration 5, loss = 1.45430147\n",
      "Iteration 6, loss = 1.40966018\n",
      "Iteration 7, loss = 1.35437537\n",
      "Iteration 8, loss = 1.33118760\n",
      "Iteration 9, loss = 1.24191987\n",
      "Iteration 10, loss = 1.18475306\n",
      "Iteration 11, loss = 1.13623856\n",
      "Iteration 12, loss = 1.09781597\n",
      "Iteration 13, loss = 1.06994020\n",
      "Iteration 14, loss = 1.04629004\n",
      "Iteration 15, loss = 1.03139447\n",
      "Iteration 16, loss = 1.02059978\n",
      "Iteration 17, loss = 1.01210405\n",
      "Iteration 18, loss = 1.00749089\n",
      "Iteration 19, loss = 1.00343588\n",
      "Iteration 20, loss = 0.99791901\n",
      "Iteration 21, loss = 0.99779786\n",
      "Iteration 22, loss = 0.99435031\n",
      "Iteration 23, loss = 0.99359331\n",
      "Iteration 24, loss = 1.00772804\n",
      "Iteration 25, loss = 0.99988960\n",
      "Iteration 26, loss = 0.99994400\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.71844476\n",
      "Iteration 2, loss = 1.54990734\n",
      "Iteration 3, loss = 1.51870513\n",
      "Iteration 4, loss = 1.49516819\n",
      "Iteration 5, loss = 1.46556526\n",
      "Iteration 6, loss = 1.43657514\n",
      "Iteration 7, loss = 1.37812418\n",
      "Iteration 8, loss = 1.31712709\n",
      "Iteration 9, loss = 1.25704212\n",
      "Iteration 10, loss = 1.19655129\n",
      "Iteration 11, loss = 1.14365653\n",
      "Iteration 12, loss = 1.21747600\n",
      "Iteration 13, loss = 1.22041361\n",
      "Iteration 14, loss = 1.18140142\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.83390582\n",
      "Iteration 2, loss = 1.58967285\n",
      "Iteration 3, loss = 1.56111264\n",
      "Iteration 4, loss = 1.55737537\n",
      "Iteration 5, loss = 1.55618355\n",
      "Iteration 6, loss = 1.55704929\n",
      "Iteration 7, loss = 1.55602747\n",
      "Iteration 8, loss = 1.55633748\n",
      "Iteration 9, loss = 1.55680544\n",
      "Iteration 10, loss = 1.55643525\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 23.59837684\n",
      "Iteration 2, loss = 19.51480421\n",
      "Iteration 3, loss = 15.96841084\n",
      "Iteration 4, loss = 17.37258634\n",
      "Iteration 5, loss = 11.01537047\n",
      "Iteration 6, loss = 12.79892391\n",
      "Iteration 7, loss = 12.57685694\n",
      "Iteration 8, loss = 16.58657134\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 23.48911071\n",
      "Iteration 2, loss = 23.31543203\n",
      "Iteration 3, loss = 23.78058424\n",
      "Iteration 4, loss = 24.00009718\n",
      "Iteration 5, loss = 19.39168322\n",
      "Iteration 6, loss = 14.59579234\n",
      "Iteration 7, loss = 16.44261528\n",
      "Iteration 8, loss = 15.56261788\n",
      "Iteration 9, loss = 13.67386774\n",
      "Iteration 10, loss = 12.90431799\n",
      "Iteration 11, loss = 14.05800745\n",
      "Iteration 12, loss = 13.71605676\n",
      "Iteration 13, loss = 13.86676438\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 23.81383652\n",
      "Iteration 2, loss = 18.99905698\n",
      "Iteration 3, loss = 11.56331730\n",
      "Iteration 4, loss = 13.91024246\n",
      "Iteration 5, loss = 13.86242758\n",
      "Iteration 6, loss = 15.55282343\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 25.66711386\n",
      "Iteration 2, loss = 22.43358867\n",
      "Iteration 3, loss = 18.65842296\n",
      "Iteration 4, loss = 15.91795025\n",
      "Iteration 5, loss = 12.15183791\n",
      "Iteration 6, loss = 11.52719622\n",
      "Iteration 7, loss = 13.57851896\n",
      "Iteration 8, loss = 14.07480376\n",
      "Iteration 9, loss = 12.99483466\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 27.84054827\n",
      "Iteration 2, loss = 23.75725000\n",
      "Iteration 3, loss = 16.08011090\n",
      "Iteration 4, loss = 16.64986802\n",
      "Iteration 5, loss = 14.30660072\n",
      "Iteration 6, loss = 11.71511416\n",
      "Iteration 7, loss = 17.77679826\n",
      "Iteration 8, loss = 18.01223604\n",
      "Iteration 9, loss = 13.90686724\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 21.72609168\n",
      "Iteration 2, loss = 16.19873107\n",
      "Iteration 3, loss = 12.54581422\n",
      "Iteration 4, loss = 11.24075263\n",
      "Iteration 5, loss = 12.09754713\n",
      "Iteration 6, loss = 12.89211031\n",
      "Iteration 7, loss = 10.80658866\n",
      "Iteration 8, loss = 12.60807173\n",
      "Iteration 9, loss = 14.14319145\n",
      "Iteration 10, loss = 13.43342864\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 27.41564672\n",
      "Iteration 2, loss = 29.01419318\n",
      "Iteration 3, loss = 28.28589850\n",
      "Iteration 4, loss = 23.64766895\n",
      "Iteration 5, loss = 13.14668812\n",
      "Iteration 6, loss = 12.63668525\n",
      "Iteration 7, loss = 16.54151486\n",
      "Iteration 8, loss = 13.79982538\n",
      "Iteration 9, loss = 17.04704375\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 27.27328977\n",
      "Iteration 2, loss = 20.31617985\n",
      "Iteration 3, loss = 14.46318496\n",
      "Iteration 4, loss = 11.97723608\n",
      "Iteration 5, loss = 12.16670540\n",
      "Iteration 6, loss = 11.59261899\n",
      "Iteration 7, loss = 12.78147658\n",
      "Iteration 8, loss = 12.26494645\n",
      "Iteration 9, loss = 15.04528565\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 26.04372127\n",
      "Iteration 2, loss = 22.65359218\n",
      "Iteration 3, loss = 19.32114291\n",
      "Iteration 4, loss = 14.85324653\n",
      "Iteration 5, loss = 12.61724917\n",
      "Iteration 6, loss = 14.62802542\n",
      "Iteration 7, loss = 13.38157978\n",
      "Iteration 8, loss = 14.27748633\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 25.95474605\n",
      "Iteration 2, loss = 28.03330260\n",
      "Iteration 3, loss = 17.55574152\n",
      "Iteration 4, loss = 14.26536230\n",
      "Iteration 5, loss = 16.73453210\n",
      "Iteration 6, loss = 15.03027876\n",
      "Iteration 7, loss = 15.47091558\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'hidden_layer_sizes': [(100,), (55, 55), (20, 20, 20)], 'activation': ['logistic', 'relu', 'tanh', 'identity'], 'alpha': [1e-06, 1e-05, 0.0001, 0.001, 0.01], 'solver': ['lbfgs', 'sgd', 'adam'], 'batch_size': [100, 200, 300], 'verbose': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpTuned.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': False,\n",
       " 'solver': 'adam',\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'batch_size': 100,\n",
       " 'alpha': 0.01,\n",
       " 'activation': 'tanh'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpTuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8258340985613712"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpTuned.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KfoldCrossVal mean score using Multi-layer Perceptron classifier is 0.8256565229948473\n",
      "Accuracy score using Multi-layer Perceptron classifier is 0.817806333180358\n"
     ]
    }
   ],
   "source": [
    "mlpN=MLPClassifier(verbose=False,solver='adam',hidden_layer_sizes=(100,),batch_size=100,alpha=0.01,activation='tanh')\n",
    "\n",
    "print(\"KfoldCrossVal mean score using Multi-layer Perceptron classifier is %s\" %cross_val_score(mlpN,X,y,cv=10).mean())\n",
    "\n",
    "\n",
    "sm= mlpN.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sm.predict(X_test)\n",
    "print(\"Accuracy score using Multi-layer Perceptron classifier is %s\" %metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which hyper-parameters are important?\n",
    "\n",
    "For nMulti-layer Perceptron classifier, verbose, solver, hidden_layer_sizes, batch_size, alpha, activation matter.\n",
    "\n",
    "* What hyper-parameter values work best? \n",
    "\n",
    "Through all results we got above, it's apparent that when 'verbose': False,'solver': 'adam', 'hidden_layer_sizes': (100,),'batch_size': 100,'alpha': 0.01,'activation': 'tanh', the accuracy is highest, 83%.\n",
    "\n",
    "* How the the neural network compare to the supervised learners in part A?   \n",
    "\n",
    "Comparing to the supervised learners in part A, the neutral network works better than naive bayes, appropriate to K-Nearest Neighbors and worse than decision tree and random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked ensemble super-model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mlxtend.StackingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[rdfN,mlpN], meta_classifier=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 0.99 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.83 (+/- 0.03) [Neutral Network]\n",
      "Accuracy: 0.99 (+/- 0.00) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([rdfN,mlpN, sclf], \n",
    "                      ['Random Forest', \n",
    "                       'Neutral Network',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cause the accuracy of random forest equals to the accuracy of stacking classifier, I could not figure out whether stacking network helps.So here I chose knn to do a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclfK = StackingClassifier(classifiers=[knn,mlpN], meta_classifier=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 0.83 (+/- 0.03) [K-Nearest Neighbors]\n",
      "Accuracy: 0.83 (+/- 0.03) [Neutral Network]\n",
      "Accuracy: 0.99 (+/- 0.00) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([knn,mlpN, sclf], \n",
    "                      ['K-Nearest Neighbors', \n",
    "                       'Neutral Network',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Did the stacked ensemble super-model help?\n",
    "\n",
    "Through the test, it's obvious that the stacked ensemble super-model helps.\n",
    "\n",
    "* How did to combine the models?  \n",
    "\n",
    "Using mlxtend.StackingClassifier() to combine the best learners in part A & B.\n",
    "\n",
    "* Cross-validate the model. How well did it do?   \n",
    "\n",
    "Through 10 fold cross validation, it's apparent that the accuracy is 0.99 which means the model's accuracy is very high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
